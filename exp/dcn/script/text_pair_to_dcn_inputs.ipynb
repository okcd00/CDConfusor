{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate DCN form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chendian/.conda/envs/pycorrector/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer('../../pretrained_models/chinese-roberta-wwm-ext/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jin', 'tian', '30', 'du', 'Ｉ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "tokens = ['今', '天', '30', '度', 'Ｉ']\n",
    "lazy_pinyin(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "721it [00:01, 421.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin\n",
    "# err, cor, 111, pinyin_indexes\n",
    "\n",
    "# cctc\n",
    "src_path = '../../data/cn/cctc/cctc_test.tsv'\n",
    "tgt_path = '../../data/cn/cctc/cctc_test.dcn.txt'\n",
    "\n",
    "# dcn\n",
    "# src_path = '../../data/cn/Wang271k_augw/dcn_train.augw.tsv'\n",
    "# tgt_path = '../../data/cn/Wang271k_augw/dcn_train.augw.dcn.txt'\n",
    "\n",
    "# rw\n",
    "# src_path = '../../data/cn/rw/rw_test.tsv'\n",
    "# tgt_path = '../../data/cn/rw/rw_test.dcn.txt'\n",
    "\n",
    "# findoc\n",
    "# src_path = '../../data/cn/findoc/findoc_test.tsv'\n",
    "# tgt_path = '../../data/cn/findoc/findoc_test.dcn.txt'\n",
    "\n",
    "vocab = [line.strip() for line in open('../vocab/vocab.txt', 'r')]\n",
    "pinyin_vocab = [line.strip() for line in open('../vocab/pinyin_vocab.txt', 'r')]\n",
    "\n",
    "\"\"\"\n",
    "英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 新 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 星 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\t359 107 333 10 133 318 10 60 0 333 164 14 194 358 80 359 107 107 131 231 61 21 186 0 357 341 362 103 359 107 226 332 357 167 283 7 79 292 130 368 339 344 247 82 334 81 63 92 292 0 113 318 387 131 131 396 274 332 10 178 79 292 98 322 137 345 193 275 61 127 120 357 134 257 342 0\n",
    "清 华 大 学 段 海 鑫 教 授 表 示 。\t清 华 大 学 段 海 新 教 授 表 示 。\t1 1 1 1 1 1 1 1 1 1 1 1\t254 120 56 350 72 109 344 131 293 16 292 0\n",
    "\"\"\"\n",
    "\n",
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def pinyin_index(pinyin_str):\n",
    "    try:\n",
    "        res = pinyin_vocab.index(pinyin_str)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return 0  # [UNK]\n",
    "\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        err = ''.join([B2Q(c) for c in err])\n",
    "        cor = ''.join([B2Q(c) for c in cor])\n",
    "        tokens = tokenizer.tokenize(err)\n",
    "        cor_tokens = tokenizer.tokenize(cor)\n",
    "        assert len(tokens) == len(cor_tokens)\n",
    "        len_index = ' '.join([str(1) for _ in range(len(tokens))])\n",
    "        py_indexes = [str(pinyin_index(_py)) for _py in lazy_pinyin(tokens)]\n",
    "        py_index = ' '.join(py_indexes)\n",
    "        if len(tokens) == len(cor_tokens) == len(py_indexes):\n",
    "            res = f\"{' '.join(tokens)}\\t{' '.join(cor_tokens)}\\t{len_index}\\t{py_index}\\n\"\n",
    "            f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19567\n",
      "['不', '料', '，', '根', '据', '「', '联', '合', '国', '」', '所', '提', '供', '的', '另', '一', '张', '报', '导', '指', '出', '，', '至', '一', '九', '五', '〇', '年', '到', '二', '〇', '##〇', '##〇', '年', '的', '婴', '儿', '出', '世', '计', '数', '一', '落', '千', '长', '。'] ['不', '料', '，', '根', '据', '「', '联', '合', '国', '」', '所', '提', '供', '的', '另', '一', '张', '报', '导', '指', '出', '，', '至', '一', '九', '五', '〇', '年', '到', '二', '〇', '##〇', '##〇', '年', '的', '婴', '儿', '出', '世', '计', '数', '一', '落', '千', '丈', '。'] ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'] ['21', '171', '0', '96', '137', '0', '169', '113', '107', '0', '309', '317', '98', '61', '174', '357', '379', '10', '60', '384', '41', '0', '384', '357', '136', '337', '174', '214', '60', '79', '174', '0', '174', '0', '174', '214', '61', '359', '79', '41', '292', '127', '294', '357', '181', '249', '379', '0']\n",
      "51006\n",
      "['二', '〇', '##〇', '九', '年', '二', '月', '十', '八', '日', '，', '我', '刚', '刚', '到', '台', '湾', '来', '的', '时', '候', '，', '我', '发', '生', '了', '一', '个', '很', '不', '好', '的', '事', '茎', '，', '让', '我', '不', '高', '兴', '也', '很', '生', '气', '，', '那', '个', '时', '候', '我', '还', '没', '开', '弑', '学', '中', '文', '。'] ['二', '〇', '##〇', '九', '年', '二', '月', '十', '八', '日', '，', '我', '刚', '刚', '到', '台', '湾', '来', '的', '时', '候', '，', '我', '发', '生', '了', '一', '个', '很', '不', '好', '的', '事', '情', '，', '让', '我', '不', '高', '兴', '也', '很', '生', '气', '，', '那', '个', '时', '候', '我', '还', '没', '开', '始', '学', '中', '文', '。'] ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'] ['79', '174', '0', '174', '136', '214', '79', '365', '292', '6', '267', '0', '336', '92', '92', '60', '311', '331', '160', '61', '292', '118', '0', '336', '80', '291', '164', '357', '94', '115', '21', '112', '61', '292', '134', '0', '262', '336', '21', '93', '345', '356', '115', '291', '247', '0', '204', '94', '292', '118', '336', '109', '190', '142', '292', '350', '385', '334', '0']\n",
      "198075\n",
      "['我', '在', '二', '〇', '##〇', '三', '年', '十', '二', '月', '去', '的', '，', '我', '去', '伊', '斯', '坦', '堡', '为', '了', '我', '想', '看', '它', '的', '古', '迹', '和', '漂', '亮', '的', '风', '景', '，', '而', '且', '它', '是', '全', '世', '界', '万', '一', '的', '城', '市', '在', '亚', '欧', '洲', '中', '。'] ['我', '在', '二', '〇', '##〇', '三', '年', '十', '二', '月', '去', '的', '，', '我', '去', '伊', '斯', '坦', '堡', '为', '了', '我', '想', '看', '它', '的', '古', '迹', '和', '漂', '亮', '的', '风', '景', '，', '而', '且', '它', '是', '全', '世', '界', '万', '一', '的', '城', '市', '在', '亚', '欧', '洲', '中', '。'] ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1'] ['336', '368', '79', '174', '0', '174', '277', '214', '292', '79', '365', '257', '61', '0', '336', '257', '357', '302', '312', '10', '333', '164', '336', '341', '143', '310', '61', '100', '127', '113', '240', '170', '61', '85', '134', '0', '79', '252', '310', '292', '258', '292', '132', '331', '357', '61', '37', '292', '368', '352', '229', '386', '385', '0']\n"
     ]
    }
   ],
   "source": [
    "# check validity\n",
    "tgt_path = '../../data/cn/Wang271k_augc/dcn_train.augc.dcn.txt'\n",
    "\n",
    "for line_idx, line in enumerate(open(tgt_path, 'r')):\n",
    "    err, cor, len_index, py_index = line.strip().split('\\t')\n",
    "    err = err.split(' ')\n",
    "    cor = cor.split(' ')\n",
    "    len_index = len_index.split(' ')\n",
    "    py_index = py_index.split(' ')\n",
    "    if len(err) == len(cor) == len(len_index) == len(py_index):\n",
    "        continue\n",
    "    print(line_idx)\n",
    "    # print(err, cor, len_index, py_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid lines\n",
    "# drop = [19567,51006,198075]\n",
    "\n",
    "for path in ['../../data/cn/Wang271k_augc/dcn_train.augc.dcn.txt', \n",
    "             '../../data/cn/Wang271k_augc/dcn_train.augc.tsv']:\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [line for idx, line in enumerate(f)]\n",
    "        with open(path+'.bak', 'w') as f_bak:\n",
    "            for line in lines:\n",
    "                f_bak.write(line)\n",
    "        lines = [line for idx, line in enumerate(lines) if idx not in drop]\n",
    "    with open(path, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "    print(f\"{path} done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate SIGHAN form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def fn(src_path, tgt_path, source='CCTC'):\n",
    "    items = [line.split('\\t')[0] for line in open(src_path, 'r')]\n",
    "    print(len(items))\n",
    "    with open(tgt_path, 'w') as f:\n",
    "        for idx, item in enumerate(items):\n",
    "            item = ''.join([B2Q(c) for c in item])\n",
    "            f.write(f\"(pid={source}-{idx})\\t{item}\\n\")\n",
    "    \n",
    "\n",
    "# src_path = '../data/cctc/cctc_test.txt'\n",
    "# tgt_path = '../data/cctc/cctc.txt'\n",
    "# fn(src_path, tgt_path, 'CCTC')\n",
    "\n",
    "src_path = '../../data/cn/rw/rw_test.tsv'\n",
    "tgt_path = '../../data/cn/rw/rw_test.sighan.txt'\n",
    "fn(src_path, tgt_path, 'rw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycorrector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2355d2d49f3eb40bf5c033ab02acb69c30aeaa545337c32bb63d47de74464e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
