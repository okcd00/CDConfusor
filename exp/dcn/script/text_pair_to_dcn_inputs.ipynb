{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate DCN form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chendian/.conda/envs/bbcm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer('../vocab/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"到涨价——降价——涨价的“循环”馊主意中去。\"\n",
    "text = \"根据美国法典15usc1595a(b)的规定，任何人（包括个人及公司），引导或协助非法的进口行为（包括进口“禁止的危险物品”）应收单罚款，罚款的金额应为进口物品的价值。\"\n",
    "text = \"对于第三方受托支付借款业务，公司应当提供的商务合同，发票及其他凭证等相关资料进行合法性及真实性审查。\"\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jin', 'tian', '30', 'du', 'Ｉ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "tokens = ['今', '天', '30', '度', 'Ｉ']\n",
    "lazy_pinyin(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6569it [00:18, 352.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin\n",
    "# err, cor, 111, pinyin_indexes\n",
    "\n",
    "\n",
    "vocab = [line.strip() for line in open('../vocab/vocab.txt', 'r')]\n",
    "pinyin_vocab = [line.strip() for line in open('../vocab/pinyin_vocab.txt', 'r')]\n",
    "\n",
    "\"\"\"\n",
    "英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 新 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 星 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\t359 107 333 10 133 318 10 60 0 333 164 14 194 358 80 359 107 107 131 231 61 21 186 0 357 341 362 103 359 107 226 332 357 167 283 7 79 292 130 368 339 344 247 82 334 81 63 92 292 0 113 318 387 131 131 396 274 332 10 178 79 292 98 322 137 345 193 275 61 127 120 357 134 257 342 0\n",
    "清 华 大 学 段 海 鑫 教 授 表 示 。\t清 华 大 学 段 海 新 教 授 表 示 。\t1 1 1 1 1 1 1 1 1 1 1 1\t254 120 56 350 72 109 344 131 293 16 292 0\n",
    "\"\"\"\n",
    "\n",
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def text_to_dcn_form(text, tokenizer):    \n",
    "    tokens = tokenizer.tokenize(''.join([B2Q(c) for c in text]))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def pinyin_index(pinyin_str):\n",
    "    try:\n",
    "        res = pinyin_vocab.index(pinyin_str)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return 0  # [UNK]\n",
    "\n",
    "\n",
    "# cctc\n",
    "# src_path = '../../data/cn/cctc/cctc_test.tsv'\n",
    "# tgt_path = '../../data/cn/cctc/cctc_test.dcn.txt'\n",
    "\n",
    "# dcn\n",
    "# src_path = '../../data/cn/Wang271k_augw_ime/dcn_train.augw1.tsv'\n",
    "# tgt_path = '../../data/cn/Wang271k_augw_ime/dcn_train.augw1.dcn.txt'\n",
    "\n",
    "# rw\n",
    "# src_path = '../../data/cn/rw/rw_test.tsv'\n",
    "# tgt_path = '../../data/cn/rw/rw_test.dcn.txt'\n",
    "\n",
    "# findoc\n",
    "src_path = '../../data/cn/findoc/findoc_test.v2.tsv'\n",
    "# tsv_path = '../../data/cn/findoc/findoc_test.v2.tsv.bak'\n",
    "tgt_path = '../../data/cn/findoc/findoc_test.v2.dcn.txt'\n",
    "\n",
    "# with open(tsv_path, 'w') as f2:\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for idx, line in tqdm(enumerate(open(src_path, 'r'))):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        err = ''.join([B2Q(c) for c in err])\n",
    "        cor = ''.join([B2Q(c) for c in cor])\n",
    "        tokens = tokenizer.tokenize(err)\n",
    "        cor_tokens = tokenizer.tokenize(cor)\n",
    "        try:\n",
    "            assert sum([o != c for o, c in zip(tokens, cor_tokens)]) < 5\n",
    "            assert len(err) == len(cor)\n",
    "            assert len(tokens) == len(cor_tokens)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(err)\n",
    "            print(cor)\n",
    "            print(list(zip(tokens, cor_tokens)))\n",
    "            continue\n",
    "        len_index = ' '.join([str(1) for _ in range(len(tokens))])\n",
    "        py_indexes = [str(pinyin_index(_py)) for _py in lazy_pinyin(tokens)]\n",
    "        py_index = ' '.join(py_indexes)\n",
    "        if len(tokens) == len(cor_tokens) == len(py_indexes):\n",
    "            res = f\"{' '.join(tokens)}\\t{' '.join(cor_tokens)}\\t{len_index}\\t{py_index}\\n\"\n",
    "            f.write(res)\n",
    "            # f2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check validity\n",
    "# tgt_path = '../../data/cn/Wang271k_augc/dcn_train.augc.dcn.txt'\n",
    "tgt_path = '../../data/cn/rw/rw_test.dcn.txt'\n",
    "\n",
    "for line_idx, line in enumerate(open(tgt_path, 'r')):\n",
    "    err, cor, len_index, py_index = line.strip().split('\\t')\n",
    "    err = err.split(' ')\n",
    "    cor = cor.split(' ')\n",
    "    len_index = len_index.split(' ')\n",
    "    py_index = py_index.split(' ')\n",
    "    if len(err) == len(cor) == len(len_index) == len(py_index):\n",
    "        continue\n",
    "    print(line_idx)\n",
    "    # print(err, cor, len_index, py_index)\n",
    "else:\n",
    "    print(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid lines\n",
    "drop = []\n",
    "# drop = [19567,51006,198075]\n",
    "\n",
    "for path in ['../../data/cn/Wang271k_augc/dcn_train.augc.dcn.txt', \n",
    "             '../../data/cn/Wang271k_augc/dcn_train.augc.tsv']:\n",
    "    lines = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = [line for idx, line in enumerate(f)]\n",
    "        with open(path+'.bak', 'w') as f_bak:\n",
    "            for line in lines:\n",
    "                f_bak.write(line)\n",
    "        lines = [line for idx, line in enumerate(lines) if idx not in drop]\n",
    "    with open(path, 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(line)\n",
    "    print(f\"{path} done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate SIGHAN form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089\n"
     ]
    }
   ],
   "source": [
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def fn(src_path, tgt_path, source='CCTC'):\n",
    "    items = [line.split('\\t')[0] for line in open(src_path, 'r')]\n",
    "    print(len(items))\n",
    "    with open(tgt_path, 'w') as f:\n",
    "        for idx, item in enumerate(items):\n",
    "            item = ''.join([B2Q(c) for c in item])\n",
    "            f.write(f\"(pid={source}-{idx})\\t{item}\\n\")\n",
    "    \n",
    "\n",
    "# src_path = '../data/cctc/cctc_test.txt'\n",
    "# tgt_path = '../data/cctc/cctc.txt'\n",
    "# fn(src_path, tgt_path, 'CCTC')\n",
    "\n",
    "src_path = '../../data/cn/rw/rw_test.tsv'\n",
    "tgt_path = '../../data/cn/rw/rw_test.sighan.txt'\n",
    "fn(src_path, tgt_path, 'rw')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbcm",
   "language": "python",
   "name": "bbcm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2355d2d49f3eb40bf5c033ab02acb69c30aeaa545337c32bb63d47de74464e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
