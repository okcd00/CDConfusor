{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer('../../pretrained_models/chinese-roberta-wwm-ext/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jin', 'tian', '30', 'du', 'Ｉ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "tokens = ['今', '天', '30', '度', 'Ｉ']\n",
    "lazy_pinyin(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [00:01, 365.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin\n",
    "# err, cor, 111, pinyin_indexes\n",
    "\n",
    "src_path = '../../data/cn/cctc/cctc_train.tsv'\n",
    "tgt_path = '../../data/cn/cctc/cctc_train.dcn.txt'\n",
    "\n",
    "vocab = [line.strip() for line in open('../vocab/vocab.txt', 'r')]\n",
    "pinyin_vocab = [line.strip() for line in open('../vocab/pinyin_vocab.txt', 'r')]\n",
    "\n",
    "\"\"\"\n",
    "英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 新 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t英 国 卫 报 今 天 报 导 ， 为 了 避 免 引 发 英 国 国 教 派 的 不 满 ， 一 项 有 关 英 国 女 王 伊 莉 莎 白 二 世 将 在 下 星 期 访 问 梵 蒂 冈 时 ， 和 天 主 教 教 宗 若 望 保 禄 二 世 共 同 举 行 弥 撒 的 计 画 已 经 取 消 。\t1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\t359 107 333 10 133 318 10 60 0 333 164 14 194 358 80 359 107 107 131 231 61 21 186 0 357 341 362 103 359 107 226 332 357 167 283 7 79 292 130 368 339 344 247 82 334 81 63 92 292 0 113 318 387 131 131 396 274 332 10 178 79 292 98 322 137 345 193 275 61 127 120 357 134 257 342 0\n",
    "清 华 大 学 段 海 鑫 教 授 表 示 。\t清 华 大 学 段 海 新 教 授 表 示 。\t1 1 1 1 1 1 1 1 1 1 1 1\t254 120 56 350 72 109 344 131 293 16 292 0\n",
    "\"\"\"\n",
    "\n",
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def pinyin_index(pinyin_str):\n",
    "    try:\n",
    "        res = pinyin_vocab.index(pinyin_str)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        return 0  # [UNK]\n",
    "\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        err = ''.join([B2Q(c) for c in err])\n",
    "        cor = ''.join([B2Q(c) for c in cor])\n",
    "        tokens = tokenizer.tokenize(err)\n",
    "        cor_tokens = tokenizer.tokenize(cor)\n",
    "        assert len(tokens) == len(cor_tokens)\n",
    "        len_index = ' '.join([str(1) for _ in range(len(tokens))])\n",
    "        py_index = ' '.join([str(pinyin_index(_py)) for _py in lazy_pinyin(tokens)])\n",
    "        res = f\"{' '.join(tokens)}\\t{' '.join(cor_tokens)}\\t{len_index}\\t{py_index}\\n\"\n",
    "        f.write(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    }
   ],
   "source": [
    "src_path = '../data/cctc/cctc_test.txt'\n",
    "tgt_path = '../data/cctc/cctc.txt'\n",
    "\n",
    "\n",
    "def B2Q(uchar):\n",
    "    \"\"\"单个字符 半角转全角\"\"\"\n",
    "    inside_code = ord(uchar)\n",
    "    if inside_code < 0x0020 or inside_code > 0x7e: # 不是半角字符就返回原来的字符\n",
    "        return uchar \n",
    "    if inside_code == 0x0020: # 除了空格其他的全角半角的公式为: 半角 = 全角 - 0xfee0\n",
    "        inside_code = 0x3000\n",
    "    else:\n",
    "        inside_code += 0xfee0\n",
    "    return chr(inside_code).upper()\n",
    "\n",
    "\n",
    "def fn(src_path, tgt_path, source='CCTC'):\n",
    "    items = [line.split('\\t')[0] for line in open(src_path, 'r')]\n",
    "    print(len(items))\n",
    "    with open(tgt_path, 'w') as f:\n",
    "        for idx, item in enumerate(items):\n",
    "            item = ''.join([B2Q(c) for c in item])\n",
    "            f.write(f\"(pid={source}-{idx})\\t{item}\\n\")\n",
    "    \n",
    "\n",
    "# src_path = '../data/cctc/cctc_test.txt'\n",
    "# tgt_path = '../data/cctc/cctc.txt'\n",
    "# fn(src_path, tgt_path, 'CCTC')\n",
    "\n",
    "src_path = '../data/findoc/findoc_collect.txt'\n",
    "tgt_path = '../data/findoc/findoc.txt'\n",
    "fn(src_path, tgt_path, 'FinDoc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycorrector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10 (default, Jun  4 2021, 14:48:32) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2355d2d49f3eb40bf5c033ab02acb69c30aeaa545337c32bb63d47de74464e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
