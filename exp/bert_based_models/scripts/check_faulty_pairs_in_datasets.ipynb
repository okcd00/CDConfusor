{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "sys.path.append('/home/chendian/BBCM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "first-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [line.strip() for line in open(\"/data/chendian/cleaned_findoc_samples/autodoc_train_1.6M.220425.txt\", 'r')]\n",
    "dev_dataset = [line.strip() for line in open(\"/data/chendian/cleaned_findoc_samples/autodoc_train_last6k.220425.txt\", 'r')]\n",
    "test_dataset = [line.strip() for line in open(\"/data/chendian/cleaned_findoc_samples/autodoc_test.220424.txt\", 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "favorite-learning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['在观测样本中随机选择一个实体，将其替换为集合中的另一个同类型实体，从而造成一个心的反事实样本。\\t在观测样本中随机选择一个实体，将其替换为集合中的另一个同类型实体，从而造成一个新的反事实样本。',\n",
       " '有一些比较新的语义搜索的前言工作，好像我们有几位同学是在做这个的，说不定能有点启发。\\t有一些比较新的语义搜索的前沿工作，好像我们有几位同学是在做这个的，说不定能有点启发。',\n",
       " '老师可能还是不太明白这个算法是如何工作的，我打算画张图给他看快。\\t老师可能还是不太明白这个算法是如何工作的，我打算画张图给他看看。',\n",
       " '我们宿舍的窗帘遮光性太差，太阳一照就行了。\\t我们宿舍的窗帘遮光性太差，太阳一照就醒了。',\n",
       " '要是针对免费了，我明天就坐两站路去麦当劳吃。\\t要是真的免费了，我明天就坐两站路去麦当劳吃。',\n",
       " '跑完步之后我贞德浑身酸疼，特别难受。\\t跑完步之后我真的浑身酸疼，特别难受。',\n",
       " '要不然这也来过分了！\\t要不然这也太过分了！',\n",
       " '（1）拟变更债券募集说明书的重要约订；\\t（1）拟变更债券募集说明书的重要约定；',\n",
       " '“付款义务人”焦易记录良好，在过往三年所有历史交易中最长回款天数为超过其相应应收账款“预期付款日”后的6个月；\\t“付款义务人”交易记录良好，在过往三年所有历史交易中最长回款天数为超过其相应应收账款“预期付款日”后的6个月；',\n",
       " '《发行人服务协议》：西指“受托人”与“登记托管机构”/“支付代理机构”签署《发行人服务协议》及对该合同的任何修改或补充。\\t《发行人服务协议》：系指“受托人”与“登记托管机构”/“支付代理机构”签署《发行人服务协议》及对该合同的任何修改或补充。']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "static-cooperative",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 27850.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('心', '新'),\n",
       " ('言', '沿'),\n",
       " ('快', '看'),\n",
       " ('行', '醒'),\n",
       " ('针对', '真的'),\n",
       " ('贞德', '真的'),\n",
       " ('来', '太'),\n",
       " ('订', '定'),\n",
       " ('焦', '交'),\n",
       " ('西', '系')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_faulty_pairs(train_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fancy-virus",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 42979.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5292/5292 [00:00<00:00, 50681.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_faulty_pairs(dataset):\n",
    "    pairs = []\n",
    "    for line in tqdm(dataset):\n",
    "        err, cor = line.split()\n",
    "        err, cor = err.strip(), cor.strip()\n",
    "        wrong_ids = [i for i, (_e, _c) in enumerate(zip(err, cor)) if _e != _c]\n",
    "\n",
    "        streak = []\n",
    "        for _i, w in enumerate(wrong_ids):\n",
    "            streak.append(w)\n",
    "            if _i != 0 and w != wrong_ids[_i-1] + 1:\n",
    "                pairs.append((\n",
    "                    ''.join([err[_w] for _w in streak]), \n",
    "                    ''.join([cor[_w] for _w in streak]), \n",
    "                ))\n",
    "                streak = []\n",
    "        else:\n",
    "            if streak:\n",
    "                pairs.append((\n",
    "                    ''.join([err[_w] for _w in streak]), \n",
    "                    ''.join([cor[_w] for _w in streak]), \n",
    "                ))\n",
    "                streak = []\n",
    "    return pairs\n",
    "\n",
    "train_pairs = get_faulty_pairs(train_dataset)\n",
    "dev_pairs = get_faulty_pairs(dev_dataset)\n",
    "test_pairs = get_faulty_pairs(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 42028.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dev_pairs = []\n",
    "\n",
    "for line in tqdm(dev_dataset):\n",
    "    err, cor = line.split()\n",
    "    err, cor = err.strip(), cor.strip()\n",
    "    wrong_ids = [i for i, (_e, _c) in enumerate(zip(err, cor)) if _e != _c]\n",
    "    \n",
    "    streak = []\n",
    "    for _i, w in enumerate(wrong_ids):\n",
    "        streak.append(w)\n",
    "        if _i != 0 and w != wrong_ids[_i-1] + 1:\n",
    "            dev_pairs.append((\n",
    "                ''.join([err[_w] for _w in streak]), \n",
    "                ''.join([cor[_w] for _w in streak]), \n",
    "            ))\n",
    "            streak = []\n",
    "    else:\n",
    "        if streak:\n",
    "            dev_pairs.append((\n",
    "                ''.join([err[_w] for _w in streak]), \n",
    "                ''.join([cor[_w] for _w in streak]), \n",
    "            ))\n",
    "            streak = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "angry-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5292/5292 [00:00<00:00, 45036.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_pairs = []\n",
    "\n",
    "for line in tqdm(test_dataset):\n",
    "    err, cor = line.split()\n",
    "    err, cor = err.strip(), cor.strip()\n",
    "    wrong_ids = [i for i, (_e, _c) in enumerate(zip(err, cor)) if _e != _c]\n",
    "    \n",
    "    streak = []\n",
    "    for _i, w in enumerate(wrong_ids):\n",
    "        streak.append(w)\n",
    "        if _i != 0 and w != wrong_ids[_i-1] + 1:\n",
    "            test_pairs.append((\n",
    "                ''.join([err[_w] for _w in streak]), \n",
    "                ''.join([cor[_w] for _w in streak]), \n",
    "            ))\n",
    "            streak = []\n",
    "    else:\n",
    "        if streak:\n",
    "            test_pairs.append((\n",
    "                ''.join([err[_w] for _w in streak]), \n",
    "                ''.join([cor[_w] for _w in streak]), \n",
    "            ))\n",
    "            streak = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "committed-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12583001328021248\n",
      "0.225974930362117\n"
     ]
    }
   ],
   "source": [
    "train_pairs_set = set(train_pairs)\n",
    "print(len(set(dev_pairs).intersection(train_pairs_set)) / len(dev_pairs))\n",
    "print(len(set(test_pairs).intersection(train_pairs_set)) / len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "heated-battle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chendian/.conda/envs/bbcm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 混淆集生成组件\n",
    "import os\n",
    "import sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "sys.path.append('/home/chendian/BBCM/')\n",
    "from bbcm.data.loaders.confusor import Confusor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ideal-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use beam method.\n",
      "Pinyin sampling mode: special.\n",
      "Token sampling mode: sort.\n",
      "Now loading pinyin2token corpus.\n",
      "Loading pinyin2token_new.pkl (1070.18MB)\n",
      "Now loading zi_sim_matrix.\n",
      "Loading zi_sim_matrix.pkl (3.03MB)\n",
      "Now loading REDscore:\n",
      "Loading ziREDscore.pkl (1.93MB)\n",
      "Now generating score matrix.\n",
      "Now Loading word freuency data:\n",
      "Loading findoc_word_frequency_score_01.pkl (19.45MB)\n",
      "Loading findoc_word2_frequency_score.pkl (281.79MB)\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB)\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB)\n"
     ]
    }
   ],
   "source": [
    "conf = Confusor(\n",
    "    cand_pinyin_num=30, \n",
    "    cos_threshold=(0., 0.75), \n",
    "    method='beam', \n",
    "    token_sample_mode='sort', \n",
    "    pinyin_sample_mode='special', \n",
    "    weight=[3, 0, 0.33],   # pinyin score, word freq score\n",
    "    conf_size=300, \n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amino-selling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6024/6024 [13:55<00:00,  7.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2872/2872 [06:02<00:00,  7.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dev_rec = {}\n",
    "dev_uncatched = []\n",
    "for err, cor in tqdm(dev_pairs):\n",
    "    cfs = conf(cor)\n",
    "    if err in cfs:\n",
    "        dev_rec[(err, cor)] = cfs.index(err)\n",
    "    else:\n",
    "        dev_uncatched.append((err, cor))\n",
    "        # dev_rec[(err, cor)] = len(cfs) * 2\n",
    "    \n",
    "test_rec = {}\n",
    "test_uncatched = []\n",
    "for err, cor in tqdm(test_pairs):\n",
    "    cfs = conf(cor)\n",
    "    if err in cfs:\n",
    "        test_rec[(err, cor)] = cfs.index(err)\n",
    "    else:\n",
    "        test_uncatched.append((err, cor))\n",
    "        # test_rec[(err, cor)] = len(cfs) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "narrow-adolescent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2567"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uncatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "rental-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17086546700942587, 16.16287722199256)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_uncatched) / (len(dev_rec)+len(dev_uncatched)), sum(dev_rec.values()) / (len(dev_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "attached-coffee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6885964912280702, 63.1887323943662)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_uncatched) / (len(test_rec)+len(test_uncatched)), sum(test_rec.values()) / (len(test_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "metric-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk = 0\n",
    "for err, cor in test_uncatched:\n",
    "    if conf.word_freq.get(cor, -0.1) == -0.1:\n",
    "        unk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "increasing-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('适当', '使得'),\n",
       " ('已经', '以及'),\n",
       " ('极限', '机械'),\n",
       " ('勾', '钩'),\n",
       " ('业', '要'),\n",
       " ('园林', '原料'),\n",
       " ('个', '改'),\n",
       " ('以及', '移交'),\n",
       " ('助工', '主管'),\n",
       " ('中环', '众合'),\n",
       " ('附中', '服装'),\n",
       " ('试点', '时代'),\n",
       " ('建设', '坚守'),\n",
       " ('资源', '自营'),\n",
       " ('置业', '制药'),\n",
       " ('程', '长'),\n",
       " ('知道', '指导'),\n",
       " ('帐', '账'),\n",
       " ('之一', '质押'),\n",
       " ('帐', '账'),\n",
       " ('育', '养'),\n",
       " ('服装', '负责'),\n",
       " ('停止停', '听证听'),\n",
       " ('止', '证'),\n",
       " ('先进', '现今'),\n",
       " ('负责', '副总'),\n",
       " ('建设', '减少'),\n",
       " ('时限', '事项'),\n",
       " ('检查', '建成'),\n",
       " ('符合', '孵化'),\n",
       " ('相应', '享有'),\n",
       " ('放假', '房价'),\n",
       " ('同志', '通知'),\n",
       " ('储', '船'),\n",
       " ('以及', '移交'),\n",
       " ('统筹', '通车'),\n",
       " ('以', '有'),\n",
       " ('以', '有'),\n",
       " ('以金', '有竞'),\n",
       " ('成', '长'),\n",
       " ('面英', '棉影'),\n",
       " ('雄战', '像众'),\n",
       " ('公司', '工商'),\n",
       " ('公司流', '供水料'),\n",
       " ('吴江', '武进'),\n",
       " ('事项', '实现'),\n",
       " ('事项', '实行'),\n",
       " ('直流', '制冷'),\n",
       " ('衍生', '延伸'),\n",
       " ('供应', '共用')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_uncatched[:50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pycorrector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10 (default, Jun  4 2021, 14:48:32) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2355d2d49f3eb40bf5c033ab02acb69c30aeaa545337c32bb63d47de74464e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
