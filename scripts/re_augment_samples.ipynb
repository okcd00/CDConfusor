{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAMPLE_FILE = \"/data/chendian/repo/DCN/data/dcn_train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281381"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for line in open(PATH_TO_SAMPLE_FILE, 'r')]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281381/281381 [00:04<00:00, 60302.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "char_level_pairs = []\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    err, cor = line.rstrip().split('\\t')[:2]\n",
    "    err = err.replace(' ', '')\n",
    "    cor = cor.replace(' ', '')\n",
    "    if err == cor:\n",
    "        continue\n",
    "    else:\n",
    "        faulty_position = []\n",
    "        for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "            if _e != _c:\n",
    "                char_level_pairs.append((_e, _c))\n",
    "\n",
    "\n",
    "ct = Counter(char_level_pairs).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-level confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char-cfs Loaded ['形近', '近音', '同部首同笔画']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272099it [00:03, 72901.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# char-level confusion from SpellGCN\n",
    "cfs_path = '../data/spellGraphs.txt'\n",
    "char_cfs = {}\n",
    "for line in open(cfs_path, 'r'):\n",
    "    l, r, t = line.strip().split('|')\n",
    "    if t in ['同音同调', '同音异调', '近音异调', '近音同调']:\n",
    "        t = '近音'\n",
    "    char_cfs.setdefault(t, {})\n",
    "    char_cfs[t].setdefault(l, [])\n",
    "    char_cfs[t][l].append(r)\n",
    "backup_cfs = deepcopy(char_cfs)\n",
    "print(\"char-cfs Loaded\", list(char_cfs.keys()))\n",
    "\n",
    "\n",
    "def char_confusor(char):\n",
    "    # always take different token\n",
    "    take = char\n",
    "    candidates = char_cfs['近音'].get(char, [char])\n",
    "    if candidates:\n",
    "        take = random.choice(candidates)\n",
    "        if take != char:\n",
    "            char_cfs['近音'][char].remove(take)\n",
    "    else:\n",
    "        if backup_cfs['近音'][char]:\n",
    "            char_cfs['近音'][char] = [_c for _c in backup_cfs['近音'][char]]\n",
    "    return take\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        cor = f\"{cor[:i]}{confusor(c)}{cor[i+1:]}\"\n",
    "    return cor\n",
    "\n",
    "dir_path = '../exp/data/cn/'\n",
    "# SIGHAN\n",
    "# src_path = dir_path + 'sighan15/sighan15_train.tsv'\n",
    "# tgt_path = dir_path + 'sighan15/sighan15_train.augc.tsv'\n",
    "\n",
    "# Wang271K + SIGHAN\n",
    "src_path = dir_path + 'Wang271k/dcn_train.tsv'\n",
    "tgt_path = dir_path + 'Wang271k/dcn_train.augc.tsv'\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        aug_err = augment_single_sample(err, cor, confusor=char_confusor)\n",
    "        f.write(f\"{aug_err}\\t{cor}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word-level confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('曰', '日'), 13648),\n",
       " (('干', '千'), 5936),\n",
       " (('末', '未'), 5100),\n",
       " (('宫', '官'), 4737),\n",
       " (('莫', '其'), 4197),\n",
       " (('[', '在'), 3668),\n",
       " (('撮', '报'), 3430),\n",
       " (('肘', '时'), 2755),\n",
       " (('墓', '基'), 2702),\n",
       " (('养', '界'), 2495),\n",
       " (('玫', '攻'), 2398),\n",
       " (('柜', '相'), 2303),\n",
       " (('顶', '项'), 2283),\n",
       " (('豪', '美'), 2086),\n",
       " (('誓', '警'), 1965),\n",
       " (('苜', '有'), 1762),\n",
       " (('罡', '是'), 1586),\n",
       " (('己', '已'), 1558),\n",
       " (('耍', '要'), 1504),\n",
       " (('他', '她'), 1426),\n",
       " (('备', '各'), 1405),\n",
       " (('驭', '取'), 1353),\n",
       " (('涠', '油'), 1301),\n",
       " (('莫', '美'), 1300),\n",
       " (('菖', '直'), 1156),\n",
       " (('的', '得'), 1152),\n",
       " (('旱', '早'), 1137),\n",
       " (('天', '夫'), 1131),\n",
       " (('找', '我'), 1125),\n",
       " (('黄', '美'), 1072)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def word_confusor(char):\n",
    "    return random.choice(char_cfs['近音'].get(char, [char]))\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        cor = f\"{cor[:i]}{confusor(c)}{cor[i+1:]}\"\n",
    "    return cor\n",
    "\n",
    "\n",
    "src_path = '../exp/data/cn/Wang271k/dcn_train.tsv'\n",
    "tgt_path = '../exp/data/cn/Wang271k/dcn_train.augc.tsv'\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        aug_err = augment_single_sample(err, cor, confusor=char_confusor)\n",
    "        f.write(f\"{aug_err}\\t{cor}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ddec1db778c3212be298b228ef4697ac84ccdac836ffbc3bb0899b0f2f4a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
