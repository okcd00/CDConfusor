{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272099"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_SAMPLE_FILE = \"/home/chendian/CDConfusor/exp/data/cn/Wang271k/dcn_train.tsv\"\n",
    "lines = [line for line in open(PATH_TO_SAMPLE_FILE, 'r')]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281381/281381 [00:04<00:00, 60302.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "char_level_pairs = []\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    err, cor = line.rstrip().split('\\t')[:2]\n",
    "    err = err.replace(' ', '')\n",
    "    cor = cor.replace(' ', '')\n",
    "    if err == cor:\n",
    "        continue\n",
    "    else:\n",
    "        faulty_position = []\n",
    "        for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "            if _e != _c:\n",
    "                char_level_pairs.append((_e, _c))\n",
    "\n",
    "\n",
    "ct = Counter(char_level_pairs).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-level confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char-cfs Loaded ['形近', '近音', '同部首同笔画']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272099it [00:03, 72901.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# char-level confusion from SpellGCN\n",
    "cfs_path = '../data/spellGraphs.txt'\n",
    "char_cfs = {}\n",
    "for line in open(cfs_path, 'r'):\n",
    "    l, r, t = line.strip().split('|')\n",
    "    if t in ['同音同调', '同音异调', '近音异调', '近音同调']:\n",
    "        t = '近音'\n",
    "    char_cfs.setdefault(t, {})\n",
    "    char_cfs[t].setdefault(l, [])\n",
    "    char_cfs[t][l].append(r)\n",
    "backup_cfs = deepcopy(char_cfs)\n",
    "print(\"char-cfs Loaded\", list(char_cfs.keys()))\n",
    "\n",
    "\n",
    "def char_confusor(char):\n",
    "    # always take different token\n",
    "    take = char\n",
    "    candidates = char_cfs['近音'].get(char, [char])\n",
    "    if candidates:\n",
    "        take = random.choice(candidates)\n",
    "        if take != char:\n",
    "            char_cfs['近音'][char].remove(take)\n",
    "    else:\n",
    "        if backup_cfs['近音'][char]:\n",
    "            char_cfs['近音'][char] = [_c for _c in backup_cfs['近音'][char]]\n",
    "    return take\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        cor = f\"{cor[:i]}{confusor(c)}{cor[i+1:]}\"\n",
    "    return cor\n",
    "\n",
    "dir_path = '../exp/data/cn/'\n",
    "# SIGHAN\n",
    "# src_path = dir_path + 'sighan15/sighan15_train.tsv'\n",
    "# tgt_path = dir_path + 'sighan15/sighan15_train.augc.tsv'\n",
    "\n",
    "# Wang271K + SIGHAN\n",
    "src_path = dir_path + 'Wang271k/dcn_train.tsv'\n",
    "tgt_path = dir_path + 'Wang271k/dcn_train.augc.tsv'\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        aug_err = augment_single_sample(err, cor, confusor=char_confusor)\n",
    "        if len(err) == len(cor):\n",
    "            f.write(f\"{aug_err}\\t{cor}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word-level confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use all-similar single-freedom method.\n",
      "Pinyin sampling mode: sort.\n",
      "Token sampling mode: sort.\n",
      "Now loading pinyin2token corpus.\n",
      "Loading pinyin2token_noname.pkl (1049.38MB) cost 56.175 seconds.\n",
      "Loading similar_pinyins.pkl (1589.0MB) cost 81.734 seconds.\n",
      "Now loading REDscore:\n",
      "Loading ziREDscore.pkl (1.97MB) cost 0.049 seconds.\n",
      "Now generating score matrix.\n",
      "Now Loading word freuency data:\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB) cost 1.506 seconds.\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB) cost 66.313 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272099it [03:02, 1494.19it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(158432, 113667)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from confusor import Confusor\n",
    "\n",
    "mapping = {}\n",
    "used_conf = {}\n",
    "\n",
    "conf = Confusor(\n",
    "    cand_pinyin_num=10, \n",
    "    cos_threshold=(0., .75), \n",
    "    method='all-similar single-freedom', \n",
    "    token_sample_mode='sort', \n",
    "    pinyin_sample_mode='sort',  # special\n",
    "    weight=[1., 0, .2],   # pinyin score, word freq score, IME ranking\n",
    "    conf_size=300, ime_weight=1,\n",
    "    debug=False)\n",
    "conf.conf_with_scores = True\n",
    "\n",
    "\n",
    "def word_confusor(word, random_select=True):\n",
    "    if word not in used_conf or len(used_conf.get(word, [])) == 0:\n",
    "        used_conf[word] = [] \n",
    "        res = conf(word)\n",
    "        min_score = min([x[1] for x in res])\n",
    "        for w, score in res:\n",
    "            used_conf[word].extend([w] * int((score - min_score) // 0.01 + 1))\n",
    "        mapping[word] = [w for w in used_conf[word]]\n",
    "    \"\"\"\n",
    "    elif len(used_conf[word]) == 0:  # all out\n",
    "        if word in mapping:\n",
    "            used_conf[word] = mapping[word]\n",
    "    \"\"\"\n",
    "    if word in used_conf[word]:\n",
    "        used_conf[word].remove(word)\n",
    "    if random_select:\n",
    "        ret = random.choice(used_conf[word])\n",
    "        used_conf[word].remove(ret)\n",
    "    else:\n",
    "        ret = used_conf[word][0]\n",
    "        used_conf[word] = used_conf[word][1:]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    es, cs, streak = \"\", \"\", []\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        if len(streak) == 0:\n",
    "            es, cs, streak = e, c, [i]\n",
    "        elif i == streak[-1]+1:\n",
    "            es += e\n",
    "            cs += c\n",
    "            streak.append(i)\n",
    "        elif i != streak[-1]+1 and len(cs) > 0:\n",
    "            cor = f\"{cor[:streak[0]]}{confusor(cs)}{cor[streak[-1]+1:]}\"\n",
    "            es, cs, streak = e, c, [i]\n",
    "    else:\n",
    "        if len(cs) > 0:\n",
    "            cor = f\"{cor[:streak[0]]}{confusor(cs)}{cor[streak[-1]+1:]}\"\n",
    "            es, cs, streak = \"\", \"\", []\n",
    "    return cor\n",
    "\n",
    "\n",
    "def augment_single_sample_jieba(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    words = jieba.lcut(err)\n",
    "    pivot = 0\n",
    "    for i, w in enumerate(words):\n",
    "        _e = err[pivot: pivot+len(w)]\n",
    "        _c = cor[pivot: pivot+len(w)]\n",
    "        if _e != _c:\n",
    "            if len(_e) >= 2:\n",
    "                for offset, (__e, __c) in enumerate(zip(_e, _c)):\n",
    "                    if __e != __c:\n",
    "                        faulty_position.append((pivot+offset, pivot+offset+1, __e, __c))\n",
    "            else:\n",
    "                faulty_position.append((pivot, pivot+len(w), _e, _c))\n",
    "        pivot += len(w)\n",
    "    for i, j, _, c in faulty_position:\n",
    "        cor = f\"{cor[:i]}{confusor(c)}{cor[j:]}\"\n",
    "    return cor\n",
    "\n",
    "\n",
    "same = 0\n",
    "faulty = 0\n",
    "src_path = '../exp/data/cn/Wang271k/dcn_train.tsv'\n",
    "tgt_path = '../exp/data/cn/Wang271k_augw/dcn_train.augw6.tsv'\n",
    "\n",
    "\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        # aug_err = augment_single_sample(err, cor, confusor=word_confusor)\n",
    "        aug_err = augment_single_sample_jieba(err, cor, confusor=word_confusor)\n",
    "        if len(aug_err) == len(cor):\n",
    "            f.write(f\"{aug_err}\\t{cor}\\n\")\n",
    "            if aug_err == cor:\n",
    "                same += 1\n",
    "            else:\n",
    "                faulty += 1\n",
    "        else:\n",
    "            print(len(aug_err), aug_err)\n",
    "            print(len(cor), cor)\n",
    "\n",
    "same, faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158432, 113667)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same, faulty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ddec1db778c3212be298b228ef4697ac84ccdac836ffbc3bb0899b0f2f4a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
