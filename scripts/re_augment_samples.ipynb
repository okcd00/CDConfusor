{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272099"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_TO_SAMPLE_FILE = \"/home/chendian/CDConfusor/exp/data/cn/Wang271k/dcn_train.tsv\"\n",
    "lines = [line for line in open(PATH_TO_SAMPLE_FILE, 'r')]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281381/281381 [00:04<00:00, 60302.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "char_level_pairs = []\n",
    "\n",
    "for line in tqdm(lines):\n",
    "    err, cor = line.rstrip().split('\\t')[:2]\n",
    "    err = err.replace(' ', '')\n",
    "    cor = cor.replace(' ', '')\n",
    "    if err == cor:\n",
    "        continue\n",
    "    else:\n",
    "        faulty_position = []\n",
    "        for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "            if _e != _c:\n",
    "                char_level_pairs.append((_e, _c))\n",
    "\n",
    "\n",
    "ct = Counter(char_level_pairs).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## char-level confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char-cfs Loaded ['形近', '近音', '同部首同笔画']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "272099it [00:03, 72901.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# char-level confusion from SpellGCN\n",
    "cfs_path = '../data/spellGraphs.txt'\n",
    "char_cfs = {}\n",
    "for line in open(cfs_path, 'r'):\n",
    "    l, r, t = line.strip().split('|')\n",
    "    if t in ['同音同调', '同音异调', '近音异调', '近音同调']:\n",
    "        t = '近音'\n",
    "    char_cfs.setdefault(t, {})\n",
    "    char_cfs[t].setdefault(l, [])\n",
    "    char_cfs[t][l].append(r)\n",
    "backup_cfs = deepcopy(char_cfs)\n",
    "print(\"char-cfs Loaded\", list(char_cfs.keys()))\n",
    "\n",
    "\n",
    "def char_confusor(char):\n",
    "    # always take different token\n",
    "    take = char\n",
    "    candidates = char_cfs['近音'].get(char, [char])\n",
    "    if candidates:\n",
    "        take = random.choice(candidates)\n",
    "        if take != char:\n",
    "            char_cfs['近音'][char].remove(take)\n",
    "    else:\n",
    "        if backup_cfs['近音'][char]:\n",
    "            char_cfs['近音'][char] = [_c for _c in backup_cfs['近音'][char]]\n",
    "    return take\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        cor = f\"{cor[:i]}{confusor(c)}{cor[i+1:]}\"\n",
    "    return cor\n",
    "\n",
    "dir_path = '../exp/data/cn/'\n",
    "# SIGHAN\n",
    "# src_path = dir_path + 'sighan15/sighan15_train.tsv'\n",
    "# tgt_path = dir_path + 'sighan15/sighan15_train.augc.tsv'\n",
    "\n",
    "# Wang271K + SIGHAN\n",
    "src_path = dir_path + 'Wang271k/dcn_train.tsv'\n",
    "tgt_path = dir_path + 'Wang271k/dcn_train.augc.tsv'\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for line in tqdm(open(src_path, 'r')):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        aug_err = augment_single_sample(err, cor, confusor=char_confusor)\n",
    "        if len(err) == len(cor):\n",
    "            f.write(f\"{aug_err}\\t{cor}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word-level confusion\n",
    "> version 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# ime = json.load(open('../data/input_candidates.google.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 364 items from /home/chendian/CDConfusor//data/red_cache.pkl\n",
      "Loaded 186258 items from /home/chendian/CDConfusor//data/cfs_cache.pkl\n",
      "Loading is_memory.json (153.94MB) cost 2.834 seconds.\n",
      "Loaded 178481 items from /home/chendian/CDConfusor//data/is_memory.json\n",
      "Loading ime_memory.json (4647.33MB) cost 283.298 seconds.\n",
      "Loaded 190194 items from /home/chendian/CDConfusor//data/ime_memory.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import json\n",
    "import jieba\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pypinyin import lazy_pinyin\n",
    "\n",
    "mapping = {}\n",
    "used_conf = {}\n",
    "\n",
    "\"\"\"\n",
    "from src.confusor import Confusor as ConfusorV1\n",
    "conf = ConfusorV1(\n",
    "    cand_pinyin_num=10, \n",
    "    cos_threshold=(0., .75), \n",
    "    method='all-similar single-freedom', \n",
    "    token_sample_mode='sort', \n",
    "    pinyin_sample_mode='sort',  # special\n",
    "    weight=[1., 0, .2],   # pinyin score, similarity score, word freq score, IME ranking\n",
    "    conf_size=300, ime_weight=1,\n",
    "    debug=False)\n",
    "conf.conf_with_scores = True\n",
    "\"\"\"\n",
    "\n",
    "from src.confusor_v2 import Confusor as ConfusorV2\n",
    "cfs = ConfusorV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_confusor(word, random_select=True):\n",
    "    if word not in used_conf or len(used_conf.get(word, [])) == 0:\n",
    "        used_conf[word] = [] \n",
    "        res = conf(word)\n",
    "        min_score = min([x[1] for x in res])\n",
    "        for w, score in res:\n",
    "            used_conf[word].extend([w] * int((score - min_score) // 0.01 + 1))\n",
    "        mapping[word] = [w for w in used_conf[word]]\n",
    "    \"\"\"\n",
    "    elif len(used_conf[word]) == 0:  # all out\n",
    "        if word in mapping:\n",
    "            used_conf[word] = mapping[word]\n",
    "    \"\"\"\n",
    "    if word in used_conf[word]:\n",
    "        used_conf[word].remove(word)\n",
    "    if random_select:\n",
    "        ret = random.choice(used_conf[word])\n",
    "        used_conf[word].remove(ret)\n",
    "    else:\n",
    "        ret = used_conf[word][0]\n",
    "        used_conf[word] = used_conf[word][1:]\n",
    "    return ret\n",
    "\n",
    "\n",
    "def word_confusor_ime(word, random_select=True):\n",
    "    py_case = lazy_pinyin(word)\n",
    "    complete_input_sequence = ''.join(py_case)\n",
    "    if word not in used_conf or len(used_conf.get(word, [])) == 0:\n",
    "        used_conf[word] = [] \n",
    "        res = []\n",
    "        for omission in range(1, len(py_case[-1])):\n",
    "            inp_seq = complete_input_sequence[:-omission]\n",
    "            if inp_seq not in ime:\n",
    "                print(f\"{inp_seq} not in IME Records.\")\n",
    "            candidates = ime[inp_seq]\n",
    "            res.extend([(c, 1 / (idx + 1)) for idx, c in enumerate(candidates) if len(c) <= len(word)])\n",
    "        for w, score in res:\n",
    "            used_conf[word].extend([w] * int(score // 0.01 + 1))\n",
    "        if len(used_conf[word]) == 0:\n",
    "            return word_confusor(word, random_select=random_select)\n",
    "    if word in used_conf[word]:\n",
    "        used_conf[word].remove(word)\n",
    "    if random_select:\n",
    "        ret = random.choice(used_conf[word])\n",
    "        used_conf[word].remove(ret)\n",
    "        if len(ret) < len(word):\n",
    "            return ret + word_confusor_ime(\n",
    "                word[len(ret):], random_select=random_select)\n",
    "    else:\n",
    "        ret = used_conf[word][0]\n",
    "        used_conf[word] = used_conf[word][1:]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_confusor_cfs(word, random_select=True):\n",
    "    if word not in used_conf or len(used_conf.get(word, [])) == 0:\n",
    "        used_conf[word] = [] \n",
    "        res = []\n",
    "        \n",
    "        candidates = cfs(word, return_score=True)\n",
    "        res.extend(\n",
    "            [(c, score) for idx, (c, score) in enumerate(candidates) \n",
    "             if len(c) == len(word)])\n",
    "\n",
    "        for w, score in res:\n",
    "            used_conf[word].extend([w] * int( (1. - score) ** 2 // 0.01 + 1))\n",
    "    \n",
    "    if word in used_conf[word]:\n",
    "        used_conf[word].remove(word)\n",
    "\n",
    "    if random_select:\n",
    "        ret = random.choice(used_conf[word])\n",
    "        used_conf[word].remove(ret)\n",
    "    else:\n",
    "        ret = used_conf[word][0]\n",
    "        used_conf[word] = used_conf[word][1:]\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def augment_single_sample(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    for i, (_e, _c) in enumerate(zip(err, cor)):\n",
    "        if _e != _c:\n",
    "            faulty_position.append((i, _e, _c))\n",
    "    es, cs, streak = \"\", \"\", []\n",
    "    for i, e, c in faulty_position:\n",
    "        assert cor[i] == c\n",
    "        if len(streak) == 0:\n",
    "            es, cs, streak = e, c, [i]\n",
    "        elif i == streak[-1]+1:\n",
    "            es += e\n",
    "            cs += c\n",
    "            streak.append(i)\n",
    "        elif i != streak[-1] + 1 and len(cs) > 0:\n",
    "            cor = f\"{cor[:streak[0]]}{confusor(cs)}{cor[streak[-1]+1:]}\"\n",
    "            es, cs, streak = e, c, [i]\n",
    "    else:\n",
    "        if len(cs) > 0:\n",
    "            cor = f\"{cor[:streak[0]]}{confusor(cs)}{cor[streak[-1]+1:]}\"\n",
    "            es, cs, streak = \"\", \"\", []\n",
    "    return cor\n",
    "\n",
    "import Pinyin2Hanzi \n",
    "def augment_single_sample_jieba(err, cor, confusor):\n",
    "    faulty_position = []\n",
    "    words = jieba.lcut(err)\n",
    "    pivot = 0\n",
    "    for i, w in enumerate(words):\n",
    "        _e = err[pivot: pivot+len(w)]\n",
    "        _c = cor[pivot: pivot+len(w)]\n",
    "        if Pinyin2Hanzi.is_chinese(_e) and _e != _c:\n",
    "            if len(_e) > 2:\n",
    "                for offset, (__e, __c) in enumerate(zip(_e, _c)):\n",
    "                    if __e != __c:\n",
    "                        faulty_position.append(\n",
    "                            (pivot+offset, pivot+offset+1, __e, __c))\n",
    "            else:\n",
    "                faulty_position.append((pivot, pivot+len(w), _e, _c))\n",
    "        pivot += len(w)\n",
    "    for i, j, _, c in faulty_position:\n",
    "        try:\n",
    "            cor = f\"{cor[:i]}{confusor(c)}{cor[j:]}\"\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(cor, cor[i:j])\n",
    "    return cor\n",
    "\n",
    "\n",
    "same, faulty = 0, 0\n",
    "src_path = '../exp/data/tmp/findoc_train.230324.tsv'\n",
    "tgt_path = '../exp/data/tmp/findoc_train.230328.augw1.tsv'\n",
    "\n",
    "\n",
    "with open(tgt_path, 'w') as f:\n",
    "    for idx, line in tqdm(enumerate(open(src_path, 'r'))):\n",
    "        err, cor = line.strip().split('\\t')\n",
    "        # aug_err = augment_single_sample(err, cor, confusor=word_confusor)\n",
    "        aug_err = augment_single_sample_jieba(err, cor, confusor=word_confusor_cfs)\n",
    "        if len(aug_err) == len(cor):\n",
    "            f.write(f\"{aug_err}\\t{cor}\\n\")\n",
    "            if aug_err == cor:\n",
    "                same += 1\n",
    "            else:\n",
    "                f.write(f\"{cor}\\t{cor}\\n\")\n",
    "                faulty += 1\n",
    "        else:\n",
    "            print(len(aug_err), aug_err)\n",
    "            print(len(cor), cor)\n",
    "            print(same, faulty)\n",
    "\n",
    "same, faulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs.ism.ime_memory['hutao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs.ism.save_memory()\n",
    "cfs.ism.update_memory_from_tmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 804M\n",
      "-rw-rw-r-- 1 chendian chendian 650M Mar 23 23:41 ime_memory.json\n",
      "-rw-rw-r-- 1 chendian chendian 154M Mar 22 19:05 memory.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /data/chendian/CDConfusor/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--',\n",
       " '27632',\n",
       " '常',\n",
       " '被',\n",
       " '用来',\n",
       " '进行',\n",
       " '稀释',\n",
       " '血管',\n",
       " '中',\n",
       " '血液',\n",
       " '的',\n",
       " '实验',\n",
       " '，',\n",
       " '可能',\n",
       " '有助于',\n",
       " '研发',\n",
       " '高血压',\n",
       " '治疗剂',\n",
       " '。']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "jieba.lcut(\"--27632常被用来进行稀释血管中血液的实验，可能有助于研发高血压治疗剂。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158432, 113667)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same, faulty"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Confusion\n",
    "> Version 2.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bbcm",
   "language": "python",
   "name": "bbcm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
