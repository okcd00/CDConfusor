{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "> Set the path to the txt file of target corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chendian/.conda/envs/bbcm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer    \n",
    "BERT_DIR_PATH = \"/data/chendian/pretrained_bert_models/chinese-macbert-base/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each line is a correct sentence text.\n",
    "PATH_TO_CORPUS = \"/data/chendian/bbcm_datasets/bbcm_corpus.txt\"\n",
    "lines = [line.strip() for line in open(PATH_TO_CORPUS, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtain corpus from Wang271k dataset.\n",
    "> take Wang271k as an example, show how to obtain a pure text corpus from csc-datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract correct sentences from `.sgml` file.\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "dump_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "\n",
    "\n",
    "def proc_confusion_item_for_corpus(item, id_prefix=\"\", id_postfix=\"\"):\n",
    "    \"\"\"\n",
    "    处理 confusion set 数据集 (AutoCorpusGeneration)\n",
    "    Args:\n",
    "        item:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    root = etree.XML(item)\n",
    "    text = root.xpath('/SENTENCE/TEXT/text()')[0]\n",
    "    mistakes = []\n",
    "    for mistake in root.xpath('/SENTENCE/MISTAKE'):\n",
    "        mistakes.append({'location': int(mistake.xpath('./LOCATION/text()')[0]) - 1,\n",
    "                         'wrong': mistake.xpath('./WRONG/text()')[0].strip(),\n",
    "                         'correction': mistake.xpath('./CORRECTION/text()')[0].strip()})\n",
    "\n",
    "    cor_text = text\n",
    "    wrong_ids = []\n",
    "\n",
    "    for mis in mistakes:\n",
    "        cor_text = f'{cor_text[:mis[\"location\"]]}{mis[\"correction\"]}{cor_text[mis[\"location\"] + 1:]}'\n",
    "        wrong_ids.append(mis['location'])\n",
    "\n",
    "    if len(text) == len(cor_text):\n",
    "        return cor_text\n",
    "    print(text)\n",
    "    print(cor_text)\n",
    "    print(len(text), len(cor_text))\n",
    "    raise ValueError()\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_confusion_data(fp):\n",
    "    # read AutoCorpusGeneration corpus released from the EMNLP2018 paper\n",
    "    with open(fp, 'r', encoding='utf8') as f:\n",
    "        item = []\n",
    "        for line in tqdm(f):\n",
    "            if line.strip().startswith('<SENT') and len(item) > 0:\n",
    "                yield ''.join(item)\n",
    "                item = [line.strip()]\n",
    "            elif line.strip().startswith('<'):\n",
    "                item.append(line.strip())\n",
    "\n",
    "\n",
    "def extract_wang271k_lines():\n",
    "    file_path = '/home/chendian/BBCM/datasets/csc/train.sgml'\n",
    "    # convertor = opencc.OpenCC('tw2sp.json')\n",
    "\n",
    "    confusion_samples = [proc_confusion_item_for_corpus(item, id_prefix='cf', id_postfix=str(_i))\n",
    "                         for _i, item in enumerate(read_confusion_data(file_path))]\n",
    "    with open(dump_path, 'w') as f:\n",
    "        for sample in tqdm(confusion_samples):\n",
    "            if sample is None:\n",
    "                continue\n",
    "            f.write(f'{sample}\\n')   \n",
    "\n",
    "\n",
    "extract_wang271k_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBCM corpus\n",
    "import pypinyin\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "dump_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "\n",
    "\n",
    "def mark_unk_wang271k_corpus():\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"/data/chendian/pretrained_bert_models/chinese-macbert-base/\")\n",
    "    count = 0\n",
    "    lines = [line.strip() for line in open(dump_path, 'r')]\n",
    "    with open(dump_path.replace('.txt', '_badlines.txt'), 'w') as f:\n",
    "        for line_idx, line in tqdm(enumerate(lines)):\n",
    "            mark_flag = []\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            tokens = [_tok[2:] if _tok.startswith('##') else _tok for _tok in tokens]\n",
    "            pinyins = pypinyin.pinyin(\n",
    "                tokens, heteronym=False, style=pypinyin.NORMAL, \n",
    "                errors=lambda _x: '[PAD]' if _x in ['[PAD]', '[CLS]', '[SEP]'] else '[UNK]')\n",
    "            pinyins = [_py[0] for _py in pinyins]\n",
    "            if '[UNK]' in tokens:\n",
    "                mark_flag.append(\"unk_token\")\n",
    "            elif sum(map(len, tokens)) != len(line):\n",
    "                mark_flag.append(\"longer_token_length\")\n",
    "            if len(pinyins) != len(tokens):\n",
    "                mark_flag.append(\"unk_pinyin\")\n",
    "            if len(mark_flag) > 0:\n",
    "                f.write(f'{line_idx}\\t{mark_flag}\\t{len(line)}:{len(tokens)}t:{len(pinyins)}p\\t{line}\\t{tokens}\\t{pinyins}\\n')            \n",
    "                count += 1\n",
    "    print(\"UNK count:\", count)\n",
    "\n",
    "\n",
    "mark_unk_wang271k_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271328it [00:00, 365596.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.93 MB\n",
      " 269619 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "src_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "dump_path = './tmp/wang271k_corpus.txt'\n",
    "drop_lines = [int(line.strip().split('\\t')[0]) \n",
    "              for line in open(src_path.replace('.txt', '_badlines.txt'), 'r')]\n",
    "\n",
    "drop_pivot = 0\n",
    "line_count = 0\n",
    "with open('./tmp/wang271k_corpus.txt', 'w') as f:\n",
    "    for i, line in tqdm(enumerate(open(src_path, 'r'))):\n",
    "        if drop_pivot < len(drop_lines):\n",
    "            if i == drop_lines[drop_pivot]:\n",
    "                drop_pivot += 1\n",
    "                continue\n",
    "        # f.write(' '.join(jieba.lcut(line.strip())))\n",
    "        f.write(f'{line.strip()}\\n')\n",
    "        line_count += 1\n",
    "\n",
    "\n",
    "from utils.file_io import get_filesize\n",
    "print(dump_path, get_filesize(dump_path), 'MB\\n', line_count, 'lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate segmentation results\n",
    "> 共现矩阵（150000 x sids）：生成 word_in_sentences 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading NER model from /home/chendian/download/stanford-corenlp-4.2.2/\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "stanford_model_path = r'/home/chendian/download/stanford-corenlp-4.2.2/'\n",
    "print(f\"Now loading NER model from {stanford_model_path}\")\n",
    "\n",
    "stanford_model = StanfordCoreNLP(stanford_model_path, lang='zh', quiet=True)\n",
    "ner_model = stanford_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('近', ('近', 'MISC', (0, 1))),\n",
      " ('三', ('三', 'NUMBER', (1, 2))),\n",
      " ('年', ('年', 'MISC', (2, 3))),\n",
      " ('及', ('及', 'MISC', (3, 4))),\n",
      " ('一', ('一', 'NUMBER', (4, 5))),\n",
      " ('期', ('期', 'O', (5, 6))),\n",
      " ('，', ('，', 'O', (6, 7))),\n",
      " ('发', ('发行人', 'O', (7, 10))),\n",
      " ('行', ('投资', 'O', (10, 12))),\n",
      " ('人', ('活动', 'O', (12, 14))),\n",
      " ('投', ('产生', 'O', (14, 16))),\n",
      " ('资', ('的', 'O', (16, 17))),\n",
      " ('活', ('现金', 'O', (17, 19))),\n",
      " ('动', ('流量', 'O', (19, 21))),\n",
      " ('产', ('净额', 'O', (21, 23))),\n",
      " ('生', ('分比', 'O', (23, 25))),\n",
      " ('的', ('为-247,445.65万', 'MONEY', (25, 38))),\n",
      " ('现', ('元', 'MONEY', (38, 39))),\n",
      " ('金', ('、', 'O', (39, 40))),\n",
      " ('流', ('-', 'O', (40, 41))),\n",
      " ('量', ('2', 'NUMBER', (41, 42))),\n",
      " ('净', ('17,705.88', 'NUMBER', (42, 51))),\n",
      " ('额', ('万元', 'MISC', (51, 53))),\n",
      " ('分', ('、', 'O', (53, 54))),\n",
      " ('比', ('-', 'O', (54, 55))),\n",
      " ('为', ('2', 'MONEY', (55, 56))),\n",
      " ('-', ('50,919.02万', 'MONEY', (56, 66))),\n",
      " ('2', ('元', 'MONEY', (66, 67))),\n",
      " ('4', ('和', 'O', (67, 68))),\n",
      " ('7', ('-137,180.96万', 'MONEY', (68, 80))),\n",
      " (',', ('元', 'MONEY', (80, 81))),\n",
      " ('4', ('。', 'O', (81, 82)))]\n"
     ]
    }
   ],
   "source": [
    "# generate masks for each line in corpus\n",
    "import numpy as np\n",
    "from utils import convert_to_unicode\n",
    "from utils.text_utils import is_chinese_char, is_whitespace, is_control, is_punctuation\n",
    "\n",
    "\n",
    "PUNC_LIST = \"，；。？！…\"\n",
    "invalid_tags = ['PERSON', 'NUMBER']  # Location, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "\n",
    "def stanford_ner(text):\n",
    "    # from urllib.parse import quote\n",
    "    # fix the issue with '%' sign for StanfordNLP\n",
    "    # data=quote(text),  # quote may bring length-changing issues\n",
    "    r_dict = ner_model._request(\n",
    "        annotators='ner', \n",
    "        data=text.replace(\"%\", \"％\"))\n",
    "    words = []\n",
    "    ner_tags = []  # ['O', 'MISC', 'LOCATION', 'GPE', 'FACILITY', 'ORGANIZATION', 'DEMONYM', 'PERSON']\n",
    "    positions = []\n",
    "    for s in r_dict['sentences']:\n",
    "        for token in s['tokens']:\n",
    "            # \"可\"\n",
    "            words.append(token['originalText'])\n",
    "            # \"O\"\n",
    "            ner_tags.append(token['ner'])\n",
    "            # (2, 3) \n",
    "            positions.append((token['characterOffsetBegin'], \n",
    "                                token['characterOffsetEnd']))\n",
    "    return list(zip(words, ner_tags, positions))\n",
    "\n",
    "\n",
    "def get_valid_position_mask(text):        \n",
    "    # using stanford ner's segmentation\n",
    "    # a list of [original_text, ner_tag, (begin, end)]\n",
    "    results = stanford_ner(text)\n",
    "    \n",
    "    # the mask is char-level only for augmentation, not the same with det_label's mask\n",
    "    text_len = len(text)\n",
    "\n",
    "    # init masks\n",
    "    word_offsets = {}\n",
    "    chn_mask = np.ones(text_len, dtype=int)\n",
    "    ner_mask = np.zeros(text_len, dtype=int)\n",
    "\n",
    "    # generate chn_mask\n",
    "    for w_idx, w in enumerate(convert_to_unicode(text)):\n",
    "        if is_whitespace(w) or is_control(w):\n",
    "            chn_mask[w_idx] = 0\n",
    "        # TODO: commas are easy to correct, maybe we can also correct them\n",
    "        elif (w in PUNC_LIST) or is_punctuation(w):\n",
    "            chn_mask[w_idx] = 0\n",
    "        elif not is_chinese_char(ord(w)):\n",
    "            chn_mask[w_idx] = 0\n",
    "    \n",
    "    # generate word_offsets and ner_mask\n",
    "    valid_flag = True\n",
    "    if results[-1][-1][-1] > text_len:  # the last word's position's ending_pos\n",
    "        print(\"Detected an invalid text:\", text)\n",
    "        valid_flag = False\n",
    "    for w_idx, (w, tag, position) in enumerate(results):\n",
    "        tokens_in_word = list(range(position[0], position[1]))  # (l, r) from stanfordNLP\n",
    "        word_offsets.update(\n",
    "            {token: tokens_in_word for token in tokens_in_word})\n",
    "        # cur_chn_mask = [1 if is_chinese_char(ord(token)) else 0 for token in convert_to_unicode(w)]\n",
    "        # if (position[1] - position[0]) == len(w) == len(cur_chn_mask):\n",
    "        #     chn_mask[position[0]: position[1]] = cur_chn_mask\n",
    "        # else:\n",
    "        #     print(w, tag, position, \n",
    "        #           (position[1] - position[0], len(w), len(cur_chn_mask)))\n",
    "        #     print(f\"Unaligned length between word <{w}> and chn_mask <{cur_chn_mask}>\")\n",
    "        if valid_flag and (tag in invalid_tags):\n",
    "            ner_mask[position[0]: position[1]] = 1\n",
    "\n",
    "    return word_offsets, ner_mask.tolist(), chn_mask.tolist()\n",
    "\n",
    "\n",
    "t = \"近三年及一期，发行人投资活动产生的现金流量净额分比为-247,445.65万元、-217,705.88万元、-250,919.02万元和-137,180.96万元。\"\n",
    "# t = \"鞍钢集团矿业公司大孤山铁矿（简称“大孤山铁矿”）采矿许可证有效期至2019年12月31日，截止2019年初大孤山铁矿采矿权扩界项目已完成各项要件，但由于未取得环评批复，无法申办采矿权扩界。\"\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(list(zip(t, stanford_ner(t))))\n",
    "# pprint(get_valid_position_mask(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/user/1013/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/user/1013/jieba.cache\n",
      "Loading model cost 0.948 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.948 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "8187it [03:06, 43.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin\n",
    "from collections import defaultdict\n",
    "from utils.file_io import dump_json, load_json\n",
    "from utils.text_utils import clean_text, is_chinese_char, is_pure_chinese_phrase\n",
    "\n",
    "\n",
    "# src_path = './tmp/wang271k_corpus.txt'\n",
    "# dump_path = './tmp/word_in_sentences_wang271k.json'\n",
    "\n",
    "src_path = ['./tmp/sighan_train.json', './tmp/sighan_dev.json']\n",
    "dump_path = './tmp/word_input_in_sentences_sighan.json'\n",
    "\n",
    "\n",
    "def read_samples(fp):\n",
    "    if isinstance(fp, list):\n",
    "        samples = []\n",
    "        for _fp in fp:\n",
    "            samples += read_samples(_fp)\n",
    "    elif fp.endswith('.txt'):\n",
    "        samples = [line.strip() for line in open(fp, 'r')]\n",
    "    elif fp.endswith('.json'):\n",
    "        samples = [item['correct_text'] for item in load_json(fp)]\n",
    "    return samples\n",
    "\n",
    "samples = read_samples(src_path)\n",
    "word_in_sentences = defaultdict(set)\n",
    "\n",
    "for idx, sample in tqdm(enumerate(samples)):\n",
    "    t = clean_text(sample)\n",
    "    word_offsets, ner_mask, chn_mask = get_valid_position_mask(t)\n",
    "    try:\n",
    "        bert_tokens = list(tokenizer.tokenize(t))\n",
    "        # chars = [tok[2:] if tok.startswith('##') else tok for tok in bert_tokens]\n",
    "        assert len(t) == len(ner_mask)\n",
    "        char_tok = [(_i, tok) for _i, tok in enumerate(t)\n",
    "                    if chn_mask[_i] == 1 and ner_mask[_i] == 0 and \n",
    "                    is_pure_chinese_phrase(tok)]\n",
    "        valid_indexes = [_i for _i, tok in char_tok]\n",
    "        char_tok = [tok for _i, tok in char_tok if tok != '[UNK]']\n",
    "        word_tok = []\n",
    "        # stanfordNLP segmentation.\n",
    "        for _i, (tok_idx, word_indexes) in enumerate(word_offsets.items()):\n",
    "            if int(tok_idx) not in valid_indexes:\n",
    "                continue\n",
    "            cur_word = ''.join([t[_i] for _i in word_indexes])\n",
    "            if cur_word not in word_tok:\n",
    "                word_tok.append(cur_word)\n",
    "        # jieba segmentation\n",
    "        pivot = 0\n",
    "        for _word in jieba.lcut(t):\n",
    "            if sum([int(i not in valid_indexes) for i in range(pivot, pivot+len(_word))]) > 0:\n",
    "                # test cases\n",
    "                pivot += len(_word)\n",
    "                continue\n",
    "            if _word not in word_tok:\n",
    "                word_tok.append(_word)\n",
    "            pivot += len(_word)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # print(list(map(len, chars)), sum(map(len, chars)), len(sample['ner_mask']))\n",
    "        print(len(t), len(ner_mask))\n",
    "        print(bert_tokens)\n",
    "        print(t)\n",
    "        continue\n",
    "    words = list(set(char_tok + word_tok))\n",
    "    words = [w for w in words \n",
    "             if is_pure_chinese_phrase(w)]\n",
    "    for w in list(set(words)):\n",
    "        word_in_sentences[w].add(idx)\n",
    "\n",
    "print(len(word_in_sentences))\n",
    "word_in_sentences = {k: sorted(v) for k, v in word_in_sentences.items()}\n",
    "dump_json(word_in_sentences, dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17282\n",
      "17282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.file_io import dump_json, load_json\n",
    "src_path = './tmp/word_in_sentences_sighan.json'\n",
    "dump_path = './tmp/word_frequency_sighan.json'\n",
    "\n",
    "word_in_sentences = load_json(src_path)\n",
    "low_freq_words = {k: v for k, v in word_in_sentences.items()}\n",
    "print(len(word_in_sentences))  # how many words involved.\n",
    "print(len(low_freq_words))  # the word in less sentences.\n",
    "\n",
    "from collections import Counter\n",
    "word_frequency = dict(Counter(\n",
    "    [len(v) if len(v) <= 50 else '>50' for k, v in low_freq_words.items()]).most_common())\n",
    "dump_json(word_frequency, dump_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original-word cover rate.\n",
    "from utils.file_io import load_json\n",
    "from utils.csc_utils import get_faulty_pair\n",
    "\n",
    "src_path = \"./tmp/sighan15_test.json\"\n",
    "test_pairs = [get_faulty_pair(line) for line in load_json(src_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 414, 0.6835748792270532)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit, total = 0, 0\n",
    "for (o, t), c in Counter(test_pairs).most_common():\n",
    "    total += 1\n",
    "    if word_in_sentences.get(o):\n",
    "        # 74% origin word in wang271k corpus. 310, 414, 0.748792270531401\n",
    "        # 80% truth word in wang271k corpus. 332, 414, 0.8019323671497585\n",
    "        # 68% origin word in sighan corpus. 283, 414, 0.6835748792270532\n",
    "        # 79% truth word in sighan corpus. 329, 414, 0.7946859903381642\n",
    "        hit += 1\n",
    "    # else:\n",
    "    #     print(o, t, c)\n",
    "hit, total, hit/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use all-similar single-freedom method.\n",
      "Pinyin sampling mode: sort.\n",
      "Token sampling mode: sort.\n",
      "Now loading pinyin2token corpus.\n",
      "Loading pinyin2token_noname.pkl (1049.38MB)\n",
      "Loading similar_pinyins.pkl (1589.0MB)\n",
      "Now loading REDscore:\n",
      "Loading ziREDscore.pkl (1.97MB)\n",
      "Now generating score matrix.\n",
      "Now Loading word freuency data:\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB)\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB)\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB)\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB)\n"
     ]
    }
   ],
   "source": [
    "from confusor.confusor import default_confusor\n",
    "cfs = default_confusor()\n",
    "cfs.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [33:51<00:00,  1.85s/it] \n"
     ]
    }
   ],
   "source": [
    "cfs.conf_size = 10000\n",
    "\n",
    "from tqdm import tqdm\n",
    "candidate_hit_rank = []\n",
    "for ori_word, cur_word in tqdm(test_pairs):\n",
    "    if not ori_word:\n",
    "        continue\n",
    "    candidates = cfs(ori_word)\n",
    "    if cur_word in candidates:\n",
    "        candidate_hit_rank.append(candidates.index(cur_word))\n",
    "    else:\n",
    "        candidate_hit_rank.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(candidate_hit_rank, bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit, total = 0, 0\n",
    "for (o, t), c in Counter(test_pairs).most_common():\n",
    "    total += 1\n",
    "    if word_in_sentences.get(o):\n",
    "        # 74% origin word in wang271k corpus. 310, 414, 0.748792270531401\n",
    "        # 80% truth word in wang271k corpus. 332, 414, 0.8019323671497585\n",
    "        # 68% origin word in sighan corpus. 283, 414, 0.6835748792270532\n",
    "        # 79% truth word in sighan corpus. 329, 414, 0.7946859903381642\n",
    "        hit += 1\n",
    "    # else:\n",
    "    #     print(o, t, c)\n",
    "hit, total, hit/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "autodoc_package = json.load(\n",
    "        open('./faulty_wording_input.json', 'r'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('bbcm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ddec1db778c3212be298b228ef4697ac84ccdac836ffbc3bb0899b0f2f4a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
