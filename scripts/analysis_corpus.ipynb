{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "> Set the path to the txt file of target corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each line is a correct sentence text.\n",
    "PATH_TO_CORPUS = \"/data/chendian/bbcm_datasets/bbcm_corpus.txt\"\n",
    "lines = [line.strip() for line in open(PATH_TO_CORPUS, 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chendian/.conda/envs/bbcm/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer    \n",
    "BERT_DIR_PATH = \"/data/chendian/pretrained_bert_models/chinese-macbert-base/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtain corpus from Wang271k dataset.\n",
    "> take Wang271k as an example, show how to obtain a pure text corpus from csc-datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract correct sentences from `.sgml` file.\n",
    "from tqdm import tqdm\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "dump_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "\n",
    "\n",
    "def proc_confusion_item_for_corpus(item, id_prefix=\"\", id_postfix=\"\"):\n",
    "    \"\"\"\n",
    "    处理 confusion set 数据集 (AutoCorpusGeneration)\n",
    "    Args:\n",
    "        item:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    root = etree.XML(item)\n",
    "    text = root.xpath('/SENTENCE/TEXT/text()')[0]\n",
    "    mistakes = []\n",
    "    for mistake in root.xpath('/SENTENCE/MISTAKE'):\n",
    "        mistakes.append({'location': int(mistake.xpath('./LOCATION/text()')[0]) - 1,\n",
    "                         'wrong': mistake.xpath('./WRONG/text()')[0].strip(),\n",
    "                         'correction': mistake.xpath('./CORRECTION/text()')[0].strip()})\n",
    "\n",
    "    cor_text = text\n",
    "    wrong_ids = []\n",
    "\n",
    "    for mis in mistakes:\n",
    "        cor_text = f'{cor_text[:mis[\"location\"]]}{mis[\"correction\"]}{cor_text[mis[\"location\"] + 1:]}'\n",
    "        wrong_ids.append(mis['location'])\n",
    "\n",
    "    if len(text) == len(cor_text):\n",
    "        return cor_text\n",
    "    print(text)\n",
    "    print(cor_text)\n",
    "    print(len(text), len(cor_text))\n",
    "    raise ValueError()\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_confusion_data(fp):\n",
    "    # read AutoCorpusGeneration corpus released from the EMNLP2018 paper\n",
    "    with open(fp, 'r', encoding='utf8') as f:\n",
    "        item = []\n",
    "        for line in tqdm(f):\n",
    "            if line.strip().startswith('<SENT') and len(item) > 0:\n",
    "                yield ''.join(item)\n",
    "                item = [line.strip()]\n",
    "            elif line.strip().startswith('<'):\n",
    "                item.append(line.strip())\n",
    "\n",
    "\n",
    "def extract_wang271k_lines():\n",
    "    file_path = '/home/chendian/BBCM/datasets/csc/train.sgml'\n",
    "    # convertor = opencc.OpenCC('tw2sp.json')\n",
    "\n",
    "    confusion_samples = [proc_confusion_item_for_corpus(item, id_prefix='cf', id_postfix=str(_i))\n",
    "                         for _i, item in enumerate(read_confusion_data(file_path))]\n",
    "    with open(dump_path, 'w') as f:\n",
    "        for sample in tqdm(confusion_samples):\n",
    "            if sample is None:\n",
    "                continue\n",
    "            f.write(f'{sample}\\n')   \n",
    "\n",
    "\n",
    "extract_wang271k_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBCM corpus\n",
    "import pypinyin\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "dump_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "\n",
    "\n",
    "def mark_unk_wang271k_corpus():\n",
    "    # tokenizer = BertTokenizer.from_pretrained(\"/data/chendian/pretrained_bert_models/chinese-macbert-base/\")\n",
    "    count = 0\n",
    "    lines = [line.strip() for line in open(dump_path, 'r')]\n",
    "    with open(dump_path.replace('.txt', '_badlines.txt'), 'w') as f:\n",
    "        for line_idx, line in tqdm(enumerate(lines)):\n",
    "            mark_flag = []\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            tokens = [_tok[2:] if _tok.startswith('##') else _tok for _tok in tokens]\n",
    "            pinyins = pypinyin.pinyin(\n",
    "                tokens, heteronym=False, style=pypinyin.NORMAL, \n",
    "                errors=lambda _x: '[PAD]' if _x in ['[PAD]', '[CLS]', '[SEP]'] else '[UNK]')\n",
    "            pinyins = [_py[0] for _py in pinyins]\n",
    "            if '[UNK]' in tokens:\n",
    "                mark_flag.append(\"unk_token\")\n",
    "            elif sum(map(len, tokens)) != len(line):\n",
    "                mark_flag.append(\"longer_token_length\")\n",
    "            if len(pinyins) != len(tokens):\n",
    "                mark_flag.append(\"unk_pinyin\")\n",
    "            if len(mark_flag) > 0:\n",
    "                f.write(f'{line_idx}\\t{mark_flag}\\t{len(line)}:{len(tokens)}t:{len(pinyins)}p\\t{line}\\t{tokens}\\t{pinyins}\\n')            \n",
    "                count += 1\n",
    "    print(\"UNK count:\", count)\n",
    "\n",
    "\n",
    "mark_unk_wang271k_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271328it [00:00, 365596.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.93 MB\n",
      " 269619 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "src_path = '/data/chendian/bbcm_datasets/bbcm_corpus.txt'\n",
    "dump_path = './tmp/wang271k_corpus.txt'\n",
    "drop_lines = [int(line.strip().split('\\t')[0]) \n",
    "              for line in open(src_path.replace('.txt', '_badlines.txt'), 'r')]\n",
    "\n",
    "drop_pivot = 0\n",
    "line_count = 0\n",
    "with open('./tmp/wang271k_corpus.txt', 'w') as f:\n",
    "    for i, line in tqdm(enumerate(open(src_path, 'r'))):\n",
    "        if drop_pivot < len(drop_lines):\n",
    "            if i == drop_lines[drop_pivot]:\n",
    "                drop_pivot += 1\n",
    "                continue\n",
    "        # f.write(' '.join(jieba.lcut(line.strip())))\n",
    "        f.write(f'{line.strip()}\\n')\n",
    "        line_count += 1\n",
    "\n",
    "\n",
    "from utils.file_io import get_filesize\n",
    "print(dump_path, get_filesize(dump_path), 'MB\\n', line_count, 'lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate segmentation results\n",
    "> 共现矩阵（150000 x sids）：生成 word_in_sentences 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading NER model from /home/chendian/download/stanford-corenlp-4.2.2/\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "stanford_model_path = r'/home/chendian/download/stanford-corenlp-4.2.2/'\n",
    "print(f\"Now loading NER model from {stanford_model_path}\")\n",
    "\n",
    "stanford_model = StanfordCoreNLP(stanford_model_path, lang='zh', quiet=True)\n",
    "ner_model = stanford_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('近', ('近', 'MISC', (0, 1))),\n",
      " ('三', ('三', 'NUMBER', (1, 2))),\n",
      " ('年', ('年', 'MISC', (2, 3))),\n",
      " ('及', ('及', 'MISC', (3, 4))),\n",
      " ('一', ('一', 'NUMBER', (4, 5))),\n",
      " ('期', ('期', 'O', (5, 6))),\n",
      " ('，', ('，', 'O', (6, 7))),\n",
      " ('发', ('发行人', 'O', (7, 10))),\n",
      " ('行', ('投资', 'O', (10, 12))),\n",
      " ('人', ('活动', 'O', (12, 14))),\n",
      " ('投', ('产生', 'O', (14, 16))),\n",
      " ('资', ('的', 'O', (16, 17))),\n",
      " ('活', ('现金', 'O', (17, 19))),\n",
      " ('动', ('流量', 'O', (19, 21))),\n",
      " ('产', ('净额', 'O', (21, 23))),\n",
      " ('生', ('分比', 'O', (23, 25))),\n",
      " ('的', ('为-247,445.65万', 'MONEY', (25, 38))),\n",
      " ('现', ('元', 'MONEY', (38, 39))),\n",
      " ('金', ('、', 'O', (39, 40))),\n",
      " ('流', ('-', 'O', (40, 41))),\n",
      " ('量', ('2', 'NUMBER', (41, 42))),\n",
      " ('净', ('17,705.88', 'NUMBER', (42, 51))),\n",
      " ('额', ('万元', 'MISC', (51, 53))),\n",
      " ('分', ('、', 'O', (53, 54))),\n",
      " ('比', ('-', 'O', (54, 55))),\n",
      " ('为', ('2', 'MONEY', (55, 56))),\n",
      " ('-', ('50,919.02万', 'MONEY', (56, 66))),\n",
      " ('2', ('元', 'MONEY', (66, 67))),\n",
      " ('4', ('和', 'O', (67, 68))),\n",
      " ('7', ('-137,180.96万', 'MONEY', (68, 80))),\n",
      " (',', ('元', 'MONEY', (80, 81))),\n",
      " ('4', ('。', 'O', (81, 82)))]\n"
     ]
    }
   ],
   "source": [
    "# generate masks for each line in corpus\n",
    "import numpy as np\n",
    "from utils import convert_to_unicode\n",
    "from utils.text_utils import is_chinese_char, is_whitespace, is_control, is_punctuation\n",
    "\n",
    "\n",
    "PUNC_LIST = \"，；。？！…\"\n",
    "invalid_tags = ['PERSON', 'NUMBER']  # Location, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "\n",
    "def stanford_ner(text):\n",
    "    # from urllib.parse import quote\n",
    "    # fix the issue with '%' sign for StanfordNLP\n",
    "    # data=quote(text),  # quote may bring length-changing issues\n",
    "    r_dict = ner_model._request(\n",
    "        annotators='ner', \n",
    "        data=text.replace(\"%\", \"％\"))\n",
    "    words = []\n",
    "    ner_tags = []  # ['O', 'MISC', 'LOCATION', 'GPE', 'FACILITY', 'ORGANIZATION', 'DEMONYM', 'PERSON']\n",
    "    positions = []\n",
    "    for s in r_dict['sentences']:\n",
    "        for token in s['tokens']:\n",
    "            # \"可\"\n",
    "            words.append(token['originalText'])\n",
    "            # \"O\"\n",
    "            ner_tags.append(token['ner'])\n",
    "            # (2, 3) \n",
    "            positions.append((token['characterOffsetBegin'], \n",
    "                                token['characterOffsetEnd']))\n",
    "    return list(zip(words, ner_tags, positions))\n",
    "\n",
    "\n",
    "def get_valid_position_mask(text):        \n",
    "    # using stanford ner's segmentation\n",
    "    # a list of [original_text, ner_tag, (begin, end)]\n",
    "    results = stanford_ner(text)\n",
    "    \n",
    "    # the mask is char-level only for augmentation, not the same with det_label's mask\n",
    "    text_len = len(text)\n",
    "\n",
    "    # init masks\n",
    "    word_offsets = {}\n",
    "    chn_mask = np.ones(text_len, dtype=int)\n",
    "    ner_mask = np.zeros(text_len, dtype=int)\n",
    "\n",
    "    # generate chn_mask\n",
    "    for w_idx, w in enumerate(convert_to_unicode(text)):\n",
    "        if is_whitespace(w) or is_control(w):\n",
    "            chn_mask[w_idx] = 0\n",
    "        # TODO: commas are easy to correct, maybe we can also correct them\n",
    "        elif (w in PUNC_LIST) or is_punctuation(w):\n",
    "            chn_mask[w_idx] = 0\n",
    "        elif not is_chinese_char(ord(w)):\n",
    "            chn_mask[w_idx] = 0\n",
    "    \n",
    "    # generate word_offsets and ner_mask\n",
    "    valid_flag = True\n",
    "    if results[-1][-1][-1] > text_len:  # the last word's position's ending_pos\n",
    "        print(\"Detected an invalid text:\", text)\n",
    "        valid_flag = False\n",
    "    for w_idx, (w, tag, position) in enumerate(results):\n",
    "        tokens_in_word = list(range(position[0], position[1]))  # (l, r) from stanfordNLP\n",
    "        word_offsets.update(\n",
    "            {token: tokens_in_word for token in tokens_in_word})\n",
    "        # cur_chn_mask = [1 if is_chinese_char(ord(token)) else 0 for token in convert_to_unicode(w)]\n",
    "        # if (position[1] - position[0]) == len(w) == len(cur_chn_mask):\n",
    "        #     chn_mask[position[0]: position[1]] = cur_chn_mask\n",
    "        # else:\n",
    "        #     print(w, tag, position, \n",
    "        #           (position[1] - position[0], len(w), len(cur_chn_mask)))\n",
    "        #     print(f\"Unaligned length between word <{w}> and chn_mask <{cur_chn_mask}>\")\n",
    "        if valid_flag and (tag in invalid_tags):\n",
    "            ner_mask[position[0]: position[1]] = 1\n",
    "\n",
    "    return word_offsets, ner_mask.tolist(), chn_mask.tolist()\n",
    "\n",
    "\n",
    "t = \"近三年及一期，发行人投资活动产生的现金流量净额分比为-247,445.65万元、-217,705.88万元、-250,919.02万元和-137,180.96万元。\"\n",
    "# t = \"鞍钢集团矿业公司大孤山铁矿（简称“大孤山铁矿”）采矿许可证有效期至2019年12月31日，截止2019年初大孤山铁矿采矿权扩界项目已完成各项要件，但由于未取得环评批复，无法申办采矿权扩界。\"\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(list(zip(t, stanford_ner(t))))\n",
    "# pprint(get_valid_position_mask(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/user/1013/jieba.cache\n",
      "DEBUG:jieba:Loading model from cache /tmp/user/1013/jieba.cache\n",
      "Loading model cost 0.948 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.948 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG:jieba:Prefix dict has been built successfully.\n",
      "8187it [03:06, 43.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "from pypinyin import lazy_pinyin\n",
    "from collections import defaultdict\n",
    "from utils.file_io import dump_json, load_json\n",
    "from utils.text_utils import clean_text, is_chinese_char, is_pure_chinese_phrase\n",
    "\n",
    "\n",
    "# src_path = './tmp/wang271k_corpus.txt'\n",
    "# dump_path = './tmp/word_in_sentences_wang271k.json'\n",
    "\n",
    "src_path = ['./tmp/sighan_train.json', './tmp/sighan_dev.json']\n",
    "dump_path = './tmp/word_input_in_sentences_sighan.json'\n",
    "\n",
    "\n",
    "def read_samples(fp):\n",
    "    if isinstance(fp, list):\n",
    "        samples = []\n",
    "        for _fp in fp:\n",
    "            samples += read_samples(_fp)\n",
    "    elif fp.endswith('.txt'):\n",
    "        samples = [line.strip() for line in open(fp, 'r')]\n",
    "    elif fp.endswith('.json'):\n",
    "        samples = [item['correct_text'] for item in load_json(fp)]\n",
    "    return samples\n",
    "\n",
    "samples = read_samples(src_path)\n",
    "word_in_sentences = defaultdict(set)\n",
    "\n",
    "for idx, sample in tqdm(enumerate(samples)):\n",
    "    t = clean_text(sample)\n",
    "    word_offsets, ner_mask, chn_mask = get_valid_position_mask(t)\n",
    "    try:\n",
    "        bert_tokens = list(tokenizer.tokenize(t))\n",
    "        # chars = [tok[2:] if tok.startswith('##') else tok for tok in bert_tokens]\n",
    "        assert len(t) == len(ner_mask)\n",
    "        char_tok = [(_i, tok) for _i, tok in enumerate(t)\n",
    "                    if chn_mask[_i] == 1 and ner_mask[_i] == 0 and \n",
    "                    is_pure_chinese_phrase(tok)]\n",
    "        valid_indexes = [_i for _i, tok in char_tok]\n",
    "        char_tok = [tok for _i, tok in char_tok if tok != '[UNK]']\n",
    "        word_tok = []\n",
    "        # stanfordNLP segmentation.\n",
    "        for _i, (tok_idx, word_indexes) in enumerate(word_offsets.items()):\n",
    "            if int(tok_idx) not in valid_indexes:\n",
    "                continue\n",
    "            cur_word = ''.join([t[_i] for _i in word_indexes])\n",
    "            if cur_word not in word_tok:\n",
    "                word_tok.append(cur_word)\n",
    "        # jieba segmentation\n",
    "        pivot = 0\n",
    "        for _word in jieba.lcut(t):\n",
    "            if sum([int(i not in valid_indexes) for i in range(pivot, pivot+len(_word))]) > 0:\n",
    "                # test cases\n",
    "                pivot += len(_word)\n",
    "                continue\n",
    "            if _word not in word_tok:\n",
    "                word_tok.append(_word)\n",
    "            pivot += len(_word)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # print(list(map(len, chars)), sum(map(len, chars)), len(sample['ner_mask']))\n",
    "        print(len(t), len(ner_mask))\n",
    "        print(bert_tokens)\n",
    "        print(t)\n",
    "        continue\n",
    "    words = list(set(char_tok + word_tok))\n",
    "    words = [w for w in words \n",
    "             if is_pure_chinese_phrase(w)]\n",
    "    for w in list(set(words)):\n",
    "        word_in_sentences[w].add(idx)\n",
    "\n",
    "print(len(word_in_sentences))\n",
    "word_in_sentences = {k: sorted(v) for k, v in word_in_sentences.items()}\n",
    "dump_json(word_in_sentences, dump_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17282\n",
      "17282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.file_io import dump_json, load_json\n",
    "src_path = './tmp/word_in_sentences_sighan.json'\n",
    "dump_path = './tmp/word_frequency_sighan.json'\n",
    "\n",
    "word_in_sentences = load_json(src_path)\n",
    "low_freq_words = {k: v for k, v in word_in_sentences.items()}\n",
    "print(len(word_in_sentences))  # how many words involved.\n",
    "print(len(low_freq_words))  # the word in less sentences.\n",
    "\n",
    "from collections import Counter\n",
    "word_frequency = dict(Counter(\n",
    "    [len(v) if len(v) <= 50 else '>50' for k, v in low_freq_words.items()]).most_common())\n",
    "dump_json(word_frequency, dump_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check original-word cover rate.\n",
    "from utils.file_io import load_json\n",
    "from utils.csc_utils import get_faulty_pair\n",
    "\n",
    "src_path = \"./tmp/sighan15_test.json\"\n",
    "test_pairs = [get_faulty_pair(line) for line in load_json(src_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283, 414, 0.6835748792270532)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit, total = 0, 0\n",
    "for (o, t), c in Counter(test_pairs).most_common():\n",
    "    total += 1\n",
    "    if word_in_sentences.get(o):\n",
    "        # 74% origin word in wang271k corpus. 310, 414, 0.748792270531401\n",
    "        # 80% truth word in wang271k corpus. 332, 414, 0.8019323671497585\n",
    "        # 68% origin word in sighan corpus. 283, 414, 0.6835748792270532\n",
    "        # 79% truth word in sighan corpus. 329, 414, 0.7946859903381642\n",
    "        hit += 1\n",
    "    # else:\n",
    "    #     print(o, t, c)\n",
    "hit, total, hit/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use all-similar single-freedom method.\n",
      "Pinyin sampling mode: sort.\n",
      "Token sampling mode: sort.\n",
      "Now loading pinyin2token corpus.\n",
      "Loading pinyin2token_noname.pkl (1049.38MB)\n",
      "Loading similar_pinyins.pkl (1589.0MB)\n",
      "Now loading REDscore:\n",
      "Loading ziREDscore.pkl (1.97MB)\n",
      "Now generating score matrix.\n",
      "Now Loading word freuency data:\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB)\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB)\n",
      "Loading wc_word_frequency_score_01.pkl (59.35MB)\n",
      "Loading wc_word2_frequency_score.pkl (2084.0MB)\n"
     ]
    }
   ],
   "source": [
    "from confusor.confusor import default_confusor\n",
    "cfs = default_confusor()\n",
    "cfs.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1100/1100 [33:51<00:00,  1.85s/it] \n"
     ]
    }
   ],
   "source": [
    "cfs.conf_size = 10000\n",
    "\n",
    "from tqdm import tqdm\n",
    "candidate_hit_rank = []\n",
    "for ori_word, cur_word in tqdm(test_pairs):\n",
    "    if not ori_word:\n",
    "        continue\n",
    "    candidates = cfs(ori_word)\n",
    "    if cur_word in candidates:\n",
    "        candidate_hit_rank.append(candidates.index(cur_word))\n",
    "    else:\n",
    "        candidate_hit_rank.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([393.,  35.,  13.,  30.,   2.,   2.,   2.,   1.,   5.,   3.,   2.,\n",
       "          2.,   1.,   1.,   3.,   6.,   4.,   3.,   2.,   5.,   1.,   2.,\n",
       "          1.,   1.,   2.,   1.,   3.,   2.,   0.,   2.,   0.,   1.,   1.,\n",
       "          0.,   0.,   0.,   1.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,\n",
       "          4.,   0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.]),\n",
       " array([-1.00000e+00,  9.79000e+00,  2.05800e+01,  3.13700e+01,\n",
       "         4.21600e+01,  5.29500e+01,  6.37400e+01,  7.45300e+01,\n",
       "         8.53200e+01,  9.61100e+01,  1.06900e+02,  1.17690e+02,\n",
       "         1.28480e+02,  1.39270e+02,  1.50060e+02,  1.60850e+02,\n",
       "         1.71640e+02,  1.82430e+02,  1.93220e+02,  2.04010e+02,\n",
       "         2.14800e+02,  2.25590e+02,  2.36380e+02,  2.47170e+02,\n",
       "         2.57960e+02,  2.68750e+02,  2.79540e+02,  2.90330e+02,\n",
       "         3.01120e+02,  3.11910e+02,  3.22700e+02,  3.33490e+02,\n",
       "         3.44280e+02,  3.55070e+02,  3.65860e+02,  3.76650e+02,\n",
       "         3.87440e+02,  3.98230e+02,  4.09020e+02,  4.19810e+02,\n",
       "         4.30600e+02,  4.41390e+02,  4.52180e+02,  4.62970e+02,\n",
       "         4.73760e+02,  4.84550e+02,  4.95340e+02,  5.06130e+02,\n",
       "         5.16920e+02,  5.27710e+02,  5.38500e+02,  5.49290e+02,\n",
       "         5.60080e+02,  5.70870e+02,  5.81660e+02,  5.92450e+02,\n",
       "         6.03240e+02,  6.14030e+02,  6.24820e+02,  6.35610e+02,\n",
       "         6.46400e+02,  6.57190e+02,  6.67980e+02,  6.78770e+02,\n",
       "         6.89560e+02,  7.00350e+02,  7.11140e+02,  7.21930e+02,\n",
       "         7.32720e+02,  7.43510e+02,  7.54300e+02,  7.65090e+02,\n",
       "         7.75880e+02,  7.86670e+02,  7.97460e+02,  8.08250e+02,\n",
       "         8.19040e+02,  8.29830e+02,  8.40620e+02,  8.51410e+02,\n",
       "         8.62200e+02,  8.72990e+02,  8.83780e+02,  8.94570e+02,\n",
       "         9.05360e+02,  9.16150e+02,  9.26940e+02,  9.37730e+02,\n",
       "         9.48520e+02,  9.59310e+02,  9.70100e+02,  9.80890e+02,\n",
       "         9.91680e+02,  1.00247e+03,  1.01326e+03,  1.02405e+03,\n",
       "         1.03484e+03,  1.04563e+03,  1.05642e+03,  1.06721e+03,\n",
       "         1.07800e+03]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(candidate_hit_rank, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit, total = 0, 0\n",
    "for (o, t), c in Counter(test_pairs).most_common():\n",
    "    total += 1\n",
    "    if word_in_sentences.get(o):\n",
    "        # 74% origin word in wang271k corpus. 310, 414, 0.748792270531401\n",
    "        # 80% truth word in wang271k corpus. 332, 414, 0.8019323671497585\n",
    "        # 68% origin word in sighan corpus. 283, 414, 0.6835748792270532\n",
    "        # 79% truth word in sighan corpus. 329, 414, 0.7946859903381642\n",
    "        hit += 1\n",
    "    # else:\n",
    "    #     print(o, t, c)\n",
    "hit, total, hit/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "autodoc_package = json.load(\n",
    "        open('./faulty_wording_input.json', 'r'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('bbcm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ddec1db778c3212be298b228ef4697ac84ccdac836ffbc3bb0899b0f2f4a26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
